import torch
import torch.nn as nn
import torch.nn.functional as F
import random
import numpy as np


class MLPEncoder(nn.Module):
    """Multi-layer perceptron encoder for feature transformation"""

    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.1):
        super(MLPEncoder, self).__init__()
        layers = []

        # First layer
        layers.append(nn.Linear(input_dim, hidden_dim))
        layers.append(nn.BatchNorm1d(hidden_dim))
        layers.append(nn.ReLU(inplace=True))
        layers.append(nn.Dropout(dropout))

        # Hidden layers
        for _ in range(num_layers - 2):
            layers.append(nn.Linear(hidden_dim, hidden_dim))
            layers.append(nn.BatchNorm1d(hidden_dim))
            layers.append(nn.ReLU(inplace=True))
            layers.append(nn.Dropout(dropout))

        # Final layer
        layers.append(nn.Linear(hidden_dim, output_dim))
        layers.append(nn.BatchNorm1d(output_dim))
        layers.append(nn.ReLU(inplace=True))

        self.encoder = nn.Sequential(*layers)

    def forward(self, x):
        # x: [B, T, J, D] -> [B*T*J, D]
        B, T, J, D = x.shape
        x = x.reshape(-1, D)
        out = self.encoder(x)
        # Reshape back: [B*T*J, out_dim] -> [B, T, J, out_dim]
        out = out.reshape(B, T, J, -1)
        return out


class VAEConfoundGenerator(nn.Module):              # â•å¾—å†çœ‹çœ‹èƒ½ä¸èƒ½è¡Œï¼Œéœ€è¦è¶³å¤Ÿè¯´æ˜å®ƒçš„æœ‰ç”¨æ€§
    """VAE to generate diverse counterfactual confounding factors"""

    def __init__(self, z_dim, hidden_dim=256, latent_dim=64):
        super(VAEConfoundGenerator, self).__init__()
        self.z_dim = z_dim
        self.latent_dim = latent_dim

        # Encoder: z_c -> latent distribution
        # z_c:çœŸå®è§‚æµ‹åˆ°çš„æ··æ‚ç‰¹å¾
        # é€æ­¥é™ç»´
        self.encoder = nn.Sequential(
            nn.Linear(z_dim, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(inplace=True),
        )
        self.mu_head = nn.Linear(hidden_dim // 2, latent_dim)       # å‡å€¼å¤´
        self.logvar_head = nn.Linear(hidden_dim // 2, latent_dim)   # å¯¹æ•°æ–¹å·®å¤´

        # Decoder: latent -> z_cf
        # é€æ­¥å‡ç»´
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim // 2),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim // 2, hidden_dim),
            nn.ReLU(inplace=True),
            nn.Linear(hidden_dim, z_dim),
            nn.Tanh()  # Bounded outputğŸ‘ˆç”¨äºæ¿€æ´»
        )

    def encode(self, z_c):
        # z_c: [B, z_dim]

        h = self.encoder(z_c)

        # mu/logVar:éšç©ºé—´çš„å‡å€¼å’Œæ–¹å·®ï¼Œç”¨äºå­¦ä¹ çœŸå®æ··æ‚ç‰¹å¾çš„åˆ†å¸ƒï¼Œä»ä¸­é‡‡æ ·ç”Ÿæˆåˆç†çš„åäº‹å®ç‰ˆæœ¬
        mu = self.mu_head(h)
        logvar = self.logvar_head(h)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)   # è®¡ç®—æ ‡å‡†å·®
        eps = torch.randn_like(std)     # ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒé‡‡æ ·å™ªå£°
        return mu + eps * std           # ç¼©æ”¾å¹³ç§»å¾—åˆ°æ½œåœ¨å˜é‡

    def forward(self, z_c):                             # è®­ç»ƒæµç¨‹
        mu, logvar = self.encode(z_c)
        z_latent = self.reparameterize(mu, logvar)      # åœ¨éšç©ºé—´ä¸­æ ¹æ®æ ‡å‡†å·®ã€æ­£æ€åˆ†å¸ƒè¿›è¡Œé‡‡æ ·é‡ç»„
        z_cf = self.decoder(z_latent)                   # ç„¶ådecode
        return z_cf, mu, logvar

    def sample_counterfactual(self, batch_size, device): # è¿™é‡Œæ˜¯ç”¨äºæ¨ç†çš„æ—¶å€™
        """Sample counterfactual confounders from prior distribution"""
        z_latent = torch.randn(batch_size, self.latent_dim).to(device)  #åœ¨å·²ç»ä»çœŸå®æ•°æ®ä¸­å­¦åˆ°çš„åˆ†å¸ƒç”¨éšæœºå™ªå£°è¿›è¡Œé‡‡æ ·ğŸ‘ˆå¦‚ä½•ç¡®ä¿æœ‰æ„ä¹‰â“
        z_cf = self.decoder(z_latent)
        return z_cf


class RegressionHead(nn.Module):
    """Regression head for Parkinson's gait score prediction"""

    def __init__(self, input_dim, hidden_dim=256, dropout=0.2):
        super(RegressionHead, self).__init__()
        self.global_pool = nn.AdaptiveAvgPool2d(1)  # Pool over T and J dimensions
        self.regressor = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(inplace=True),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim // 2, 1)  # Single score output
        )

    def forward(self, x):
        # x: [B, T, J, D]
        B, T, J, D = x.shape
        # Global average pooling over temporal and joint dimensions
        x = x.mean(dim=[1, 2])  # [B, D]
        score = self.regressor(x)  # [B, 1]
        return score.squeeze(-1)  # [B]


class CounterfactualCausalModeling(nn.Module):
    """
    Counterfactual causal modeling for Parkinson's gait assessment

    Core idea: Answer "What would be the gait score if the confounding factors were different?"
    without explicitly enumerating all possible confounding combinations.

    Architecture:
    - Disease-related encoder: extracts gait patterns related to Parkinson's disease
    - Confounding encoder: captures nuisance factors (age, gender, camera angle, etc.)
    - VAE generator: learns the distribution of confounding factors for counterfactual sampling
    - Factual branch: real-world prediction
    - Counterfactual branch: "what-if" prediction under different confounding scenarios
    """

    def __init__(self, backbone,input_dim=512, hidden_dim=256, z_dim=128,
                 counterfactual_strategy='learned_prior',
                 use_vae_generator=True, use_disentanglement_loss=True):
        super(CounterfactualCausalModeling, self).__init__()
        self.backbone = backbone
        self.input_dim = input_dim
        self.z_dim = z_dim
        self.counterfactual_strategy = counterfactual_strategy
        self.use_vae_generator = use_vae_generator
        self.use_disentanglement_loss = use_disentanglement_loss

        # === Disease-related and Confounding Encoders === #
        self.disease_encoder = MLPEncoder(
            input_dim=input_dim,
            hidden_dim=hidden_dim,
            output_dim=z_dim,
            num_layers=3,
            dropout=0.1
        )

        self.confound_encoder = MLPEncoder(
            input_dim=input_dim,
            hidden_dim=hidden_dim,
            output_dim=z_dim,
            num_layers=3,
            dropout=0.1
        )

        # === Counterfactual Generation === #
        if self.use_vae_generator:
            self.confound_generator = VAEConfoundGenerator(
                z_dim=z_dim,
                hidden_dim=256,
                latent_dim=64
            )

        # === Shared Regression Head === #
        self.regressor = RegressionHead(
            input_dim=z_dim * 2,  # disease + confound features
            hidden_dim=256,
            dropout=0.2
        )

        # === Domain Classifier for Disentanglement === #
        if self.use_disentanglement_loss:
            self.domain_classifier = nn.Sequential(
                nn.Linear(z_dim, 128),
                nn.ReLU(inplace=True),
                nn.Linear(128, 64),
                nn.ReLU(inplace=True),
                nn.Linear(64, 2)  # PD vs Normal
            )

    def generate_counterfactual_confounders(self, z_c_real, labels, strategy=None):     #å¯ä»¥åœ¨ç”Ÿæˆåäº‹å®æ··æ·†ç‰¹å¾æ—¶æ‰‹åŠ¨æŒ‡å®šç”Ÿæˆæ–¹å¼ï¼Œå¦åˆ™é€šè¿‡ç±»é»˜è®¤è¿›è¡Œ
        """
        Generate counterfactual confounding factors

        Args:
            z_c_real: Real confounding features [B, T, J, z_dim]
            labels: Disease labels [B] (1=PD, 0=Normal)
            strategy: Override default strategy

        Returns:
            z_cf: Counterfactual confounding features [B, T, J, z_dim]
        """
        B, T, J, z_dim = z_c_real.shape
        device = z_c_real.device
        strategy = strategy or self.counterfactual_strategy

        z_c_pooled = z_c_real.mean(dim=[1, 2])  # è¿›è¡Œæ—¶é—´å’Œå…³èŠ‚ç»´åº¦ä¸Šçš„æ± åŒ–


        if strategy == 'vae_sampling':
            # Use VAE to generate diverse counterfactuals
            if self.use_vae_generator:
                z_cf_pooled = self.confound_generator.sample_counterfactual(B, device)  # VAEç¼–ç å­¦ä¹ ï¼Œæœ‰æ„ä¹‰â“
            else:
                # Fallback to random sampling
                z_cf_pooled = torch.randn_like(z_c_pooled) * 0.5                        # éšæœºsamplingï¼Œä¸ä¸€å®šæœ‰æ„ä¹‰


        else:
            raise ValueError(f"Unknown counterfactual strategy: {strategy}")

        # Reshape back to [B, T, J, z_dim]
        z_cf = z_cf_pooled.unsqueeze(1).unsqueeze(1).expand(-1, T, J, -1)               #æ¢å¤æ—¶é—´å’Œå…³èŠ‚ç»´åº¦
        return z_cf

    def compute_disentanglement_loss(self, z_g, z_c, labels):                           # â“è¿™ä¸ªå¾—çœ‹ä»”ç»†
        """
        Encourage disentanglement: disease features should not predict confounders
        """
        # Pool features
        z_g_pooled = z_g.mean(dim=[1, 2])  # [B, z_dim]
        z_c_pooled = z_c.mean(dim=[1, 2])  # [B, z_dim]

        # Disease features shouldn't predict domain (should be confused)
        domain_pred = self.domain_classifier(z_g_pooled)

        # Create uniform labels to confuse the classifier
        uniform_labels = torch.randint_like(labels, 0, 2)
        disentangle_loss = F.cross_entropy(domain_pred, uniform_labels)

        # Orthogonality loss: encourage z_disease âŠ¥ z_confound
        z_disease_norm = F.normalize(z_g_pooled, p=2, dim=1)
        z_confound_norm = F.normalize(z_c_pooled, p=2, dim=1)
        orthogonal_loss = torch.mean((z_disease_norm * z_confound_norm).sum(dim=1) ** 2)

        return disentangle_loss + 0.1 * orthogonal_loss

    def forward(self, inputs, labels=None, intervention_type=None):
        """

        inputs:backbone outputs feature, [B, T, J=17, D=512] from MotionAGFormer
        labels:0,1,2
        intervention_type: currently only "vae_sample"

        Returns:
            Dictionary with factual/counterfactual predictions and features
        """
        inputs=self.backbone(inputs)        #å…ˆç»è¿‡backboneè·‘ä¸€æ³¢


        # === ç‰¹å¾è§£è€¦ === #
        z_g = self.disease_encoder(inputs)      # æ­¥æ€ç‰¹å¾ä¸­ä¸å¸•é‡‘æ£®æ­¥æ€è¯„åˆ†æœ‰å…³çš„ç‰¹å¾ï¼Œç”±ç–¾ç—…ç‰¹å¾ç¼–ç å™¨è¾“å‡º
        z_c = self.confound_encoder(inputs)     # æ­¥æ€ç‰¹å¾ä¸­çš„æ··æ·†å˜é‡ç‰¹å¾ï¼Œç”±æ··æ·†ç‰¹å¾ç¼–ç å™¨è¾“å‡º

        # === äº‹å®åˆ†æ”¯ === #
        Z_f = torch.cat([z_g, z_c], dim=-1)  # å°†ä¸¤ä¸ªç¼–ç å™¨çš„ç‰¹å¾è¾“å‡ºçš„ç»“æœæ‹¼æ¥é€å…¥å›å½’å¤´
        Y_f = self.regressor(Z_f)  # [B]

        # === åäº‹å®åˆ†æ”¯ === #
        z_cf = (self.
        generate_counterfactual_confounders(        # ç»™å®šäº‹å®ä¸Šçš„æ··æ·†ç‰¹å¾z_cï¼Œè¿›è¡Œåäº‹å®æ··æ·†ç‰¹å¾çš„ç”Ÿæˆ
            z_c, labels, strategy=intervention_type # ç›®å‰åªæ”¯æŒvaeé‡‡æ ·ç­–ç•¥
        ))
        Z_cf = torch.cat([z_g, z_cf], dim=-1)
        Y_cf = self.regressor(Z_cf)

        # === å¹²é¢„æ•ˆæœè¯„ä¼° === #
        # ä¸Šé¢çš„å¹²é¢„èµ·äº†å¤šå¤§ä½œç”¨ï¼Ÿé€šè¿‡å›å½’åˆ†æ•°çš„å·®å€¼æ¥è¡¡é‡
        individual_effect = Y_f - Y_cf

        # === è¿”å›å€¼å­—å…¸ === #
        retval = {
            'factual_pred': Y_f,
            'counterfactual_pred': Y_cf,
            'individual_effect': individual_effect,

            'disease_features': z_g,
            'confound_features': z_c,
            'counterfactual_confound': z_cf

        }

        # === VAEæŸå¤±  === #
        if self.use_vae_generator and self.training:
            z_confound_pooled = z_c.mean(dim=[1, 2])  # äº‹å®æ··æ·†ç‰¹å¾çš„æ± åŒ–ç»“æœ
            z_cf_vae, mu, logvar = self.confound_generator(z_confound_pooled)       # VAEå‰é¦ˆè¾“å‡ºçš„z_cfæ··æ·†å˜é‡

            # VAE losses                                                            #åäº‹å®å¹²é¢„è¦æ±‚ï¼šå¿…é¡»è¦å¯¹åº”ç°å®ä¸­å¯èƒ½å­˜åœ¨çš„çŠ¶æ€ï¼Œæ»¡è¶³äº†æŸäº›å˜é‡çš„æ”¹å˜ï¼Œè€Œéæ— æ„ä¹‰çš„ç”Ÿæˆ;ä¸‹é¢è¿™ä¸¤ä¸ªæŸå¤±å°è¯•ç¡®ä¿è¿™ä¸€ç‚¹
            recon_loss = F.mse_loss(z_cf_vae, z_confound_pooled)                    # é‡æ„æŸå¤±ï¼Œç¡®ä¿z_cfä¸åŸå§‹è¾“å…¥åœ¨ç‰¹å¾ç©ºé—´æ¥è¿‘ï¼Ÿä¸çœŸå®æ ·æœ¬åŒåˆ†å¸ƒï¼Œé˜²æ­¢ç”Ÿæˆä¸åˆç†çš„æ ·æœ¬ğŸ‘ˆå¦‚æœä¸é€‚ç”¨å®ƒï¼ŒVAEå®¹æ˜“ç”Ÿæˆå®‰å…¨ä½†æ— æ„ä¹‰çš„åäº‹å®ï¼Œä¾‹å¦‚å…¨é›¶å‘é‡ï¼Œä½†æ˜¯é‚£æ ·æ²¡æœ‰æ„ä¹‰
            kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / mu.shape[0]    # KLæ•£åº¦æŸå¤±ï¼Œçº¦æŸæ½œåœ¨ç©ºé—´æœä»æ ‡å‡†æ­£æ€åˆ†å¸ƒ

            retval['vae_recon_loss'] = recon_loss
            retval['vae_kld_loss'] = kld_loss

        # === è§£è€¦æŸå¤± === #
        if self.use_disentanglement_loss and self.training and labels is not None:
            disentangle_loss = self.compute_disentanglement_loss(z_g, z_c, labels)
            retval['disentanglement_loss'] = disentangle_loss

        return retval

    def predict_counterfactual(self, inputs, intervention="healthy"):               #è¯„ä¼°æµç¨‹ï¼Œå›ç­”â€œå¦‚æœå®ƒæ˜¯æ­£å¸¸äººâ€ï¼Œæˆ–è€…â€œå¦‚æœå®ƒæ˜¯å¸•é‡‘æ£®æ‚£è€…â€
        """
        Predict counterfactual outcomes for different interventions

        Args:
            inputs: Input features [B, T, J, D]
            intervention: Type of intervention ("healthy", "disease", "vae_sample")

        Returns:
            Counterfactual predictions
        """
        self.eval()
        with torch.no_grad():
            B = inputs.shape[0]
            device = inputs.device

            # Dummy labels for intervention
            if intervention == "healthy":
                labels = torch.ones(B).to(device)  # Assume all are PD, intervene to healthy
            else:
                labels = torch.zeros(B).to(device)  # Assume all are healthy, intervene to disease

            result = self.forward(inputs, labels, intervention_type=intervention)
            return result['counterfactual_pred']


# === Counterfactual Loss Function === #
class CounterfactualLoss(nn.Module):
    """Loss function for counterfactual causal modeling"""

    def __init__(self, lambda_consistency=0.5, lambda_vae=0.1, lambda_disentangle=0.2):
        super(CounterfactualLoss, self).__init__()
        self.lambda_consistency = lambda_consistency
        self.lambda_vae = lambda_vae
        self.lambda_disentangle = lambda_disentangle

        self.mse_loss = nn.MSELoss()

    def forward(self, model_output, labels):
        """
        Compute counterfactual causal loss

        Args:
            model_output: Output from CounterfactualCausalModeling
            labels: Ground truth gait scores [B]

        Returns:
            Dictionary of losses
        """
        losses = {}
        total_loss = 0

        # === Primary Regression Loss === #
        factual_pred = model_output['factual_pred']
        regression_loss = self.mse_loss(factual_pred, labels)
        losses['regression_loss'] = regression_loss
        total_loss += regression_loss

        # === Counterfactual Consistency Loss === #
        # Encourage meaningful but not too large differences between factual/counterfactual
        individual_effect = model_output['individual_effect']

        # Prevent trivial solutions (effect should not be zero)
        effect_magnitude = torch.mean(torch.abs(individual_effect))
        consistency_loss = torch.mean(individual_effect ** 2) - 0.1 * effect_magnitude

        losses['consistency_loss'] = consistency_loss
        total_loss += self.lambda_consistency * consistency_loss

        # === VAE Losses === #
        if 'vae_recon_loss' in model_output:
            vae_loss = model_output['vae_recon_loss'] + 0.1 * model_output['vae_kld_loss']
            losses['vae_loss'] = vae_loss
            total_loss += self.lambda_vae * vae_loss

        # === Disentanglement Loss === #
        if 'disentanglement_loss' in model_output:
            disentangle_loss = model_output['disentanglement_loss']
            losses['disentanglement_loss'] = disentangle_loss
            total_loss += self.lambda_disentangle * disentangle_loss

        losses['total_loss'] = total_loss
        return losses

#
# # === Usage Example === #
# if __name__ == "__main__":
#     # Model configuration
#     model = CounterfactualCausalModeling(
#         input_dim=512,
#         hidden_dim=256,
#         z_dim=128,
#         counterfactual_strategy='vae_sampling',
#         use_vae_generator=True,
#         use_disentanglement_loss=False
#     )
#
#     # Loss function
#     criterion = CounterfactualLoss(
#         lambda_consistency=0.5,
#         lambda_vae=0.1,
#         lambda_disentangle=0.2
#     )
#
#     # Dummy data
#     batch_size, seq_len, num_joints, feat_dim = 20, 243, 17, 512
#     inputs = torch.randn(batch_size, seq_len, num_joints, feat_dim)
#     labels = torch.randn(batch_size)  # Parkinson's gait scores
#     disease_labels = torch.randint(0, 2, (batch_size,))  # 0=Normal, 1=PD
#
#     # Training
#     model.train()
#     output = model(inputs, disease_labels)
#     losses = criterion(output, labels)
#
#     print("=== Training Results ===")
#     print("Losses:", {k: f"{v.item():.4f}" for k, v in losses.items()})
#
#     # Counterfactual Inference
#     model.eval()
#     cf_disease = model.predict_counterfactual(inputs, intervention="vae_sampling")
#
#     print("\n=== Counterfactual Predictions ===")
#     print(f"Original scores: {output['factual_pred'].detach()}")
#     print(f"If VAE sampled: {cf_disease}")
#


