# baseline





# åºå›å½’ã€å•ç—…ç†å»ºæ¨¡

**å…ˆåªå»ºæ¨¡ç—…ç†å› ç´ $z_g$**ï¼Œç›®æ ‡æ˜¯æŠŠ$z_g$è¿›è¡Œä»»åŠ¡å¯¼å‘ï¼Œè®©$z_g$æå–å‡ºæ¥èƒ½æ›´å¥½åˆ†ç±»ã€‚



---

ç”¨ä¸€ä¸ªç»“æ„ç®€å•ï¼ˆå¯èƒ½å»ºæ¨¡èƒ½åŠ›ä¸å¤ªè¶³ï¼‰çš„åºå›å½’å¤´ï¼š

```python

class OrdinalHead(nn.Module):
    '''
    target = torch.zeros(y.size(0), K-1).to(y.device)
    for j in range(1, K):
        target[:, j-1] = (y >= j).float()
    loss = F.binary_cross_entropy_with_logits(logits, target)
    '''
    def __init__(self, input_dim, hidden_dim, num_classes, dropout=0.2):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
        )
        self.out = nn.Linear(hidden_dim, num_classes - 1)  # K-1 logits

    def forward(self, x):
        feats = self.fc(x)
        logits = self.out(feats)  # [B, K-1]
        return logits

```



```python

class CounterfactualCausalModeling(nn.Module):
    """
    Stage1: base model
    """

    def __init__(self, backbone,input_dim=512, hidden_dim=256, z_dim=128):
        super(CounterfactualCausalModeling, self).__init__()
        self.backbone = backbone
        self.input_dim = input_dim
        self.z_dim = z_dim

        # === Disease-related and Confounding Encoders === #
        self.disease_encoder = MLPEncoder(
            input_dim=input_dim,
            hidden_dim=hidden_dim,
            output_dim=z_dim,
            num_layers=3,
            dropout=0.1
        )
        
        
	 def forward(self, inputs, labels=None):
        # === backbone features ===
        features = self.backbone(inputs)  # [B, T, J, C]

        # === encoded features ===
        # ç—…ç†ç‰¹å¾å’Œæ··æ·†ç‰¹å¾éƒ½ä»åŒä¸€ä¸ªbackboneå‡ºæ¥
        z_g = self.disease_encoder(features)  # [B, z_dim]

        # === ordinal prediction ===
        z_g = z_g.mean(dim=(1, 2))
        logits = self.regressor(z_g)  # [B, K-1]

        out = {
            "logits": logits,
            "disease_features": z_g
        }
        
    
```



æŸå¤±è®¡ç®—ï¼š

```python

def coral_loss(logits, labels, num_classes):
    """
    logits: [B, K-1]
    labels: [B] int64
    """
    # æ„é€  CORAL target
    target = torch.zeros(labels.size(0), num_classes - 1, device=labels.device)
    for j in range(1, num_classes):
        target[:, j-1] = (labels >= j).float()

    return F.binary_cross_entropy_with_logits(logits, target)




x, y = x.to(device), y.to(device).long()

outputs = model(x)                        # forward
logits = outputs["logits"]                # [B, K-1]

loss = coral_loss(logits, y, num_classes)


```



ç»“æœï¼š

```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.60      0.65      0.63      1026
           1       0.45      0.50      0.48       828
           2       0.21      0.12      0.16       486

    accuracy                           0.49      2340
   macro avg       0.42      0.43      0.42      2340
weighted avg       0.46      0.49      0.47      2340
```



```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.67      0.72      0.69      1026
           1       0.51      0.44      0.47       828
           2       0.26      0.28      0.27       486

    accuracy                           0.53      2340
   macro avg       0.48      0.48      0.48      2340
weighted avg       0.53      0.53      0.53      2340
```









# åºå›å½’ç»“æ„å¢å¼º

åŒä¸Šï¼Œåªæ›´æ”¹äº†åºå›å½’å¤´çš„ç»“æ„ï¼Œä½¿å…¶è¡¨è¾¾èƒ½åŠ›å’ŒåŸæœ¬çš„ClassifierHeadå¯¹é½ï¼š

```python

class OrdinalHead(nn.Module):
    def __init__(self, input_dim, hidden_dims, num_classes, dropout=0.2):
        super().__init__()
        dims = [input_dim, hidden_dims, num_classes]
        self.fcs = nn.ModuleList()
        self.bns = nn.ModuleList()
        for i in range(len(dims) - 1):
            self.fcs.append(nn.Linear(dims[i], dims[i+1]))
            self.bns.append(nn.BatchNorm1d(dims[i+1]))
        self.out = nn.Linear(dims[-1], num_classes - 1)
        self.dropout = nn.Dropout(dropout)
        self.act = nn.ReLU()

    def forward(self, x):
        for fc, bn in zip(self.fcs, self.bns):
            x = self.act(bn(fc(x)))
            x = self.dropout(x)
        logits = self.out(x)
        return logits

```



ç»“æœï¼šå·®äº†éå¸¸å¤š

```
           0       0.51      0.41      0.46      1026
           1       0.32      0.46      0.38       828
           2       0.25      0.16      0.20       486

    accuracy                           0.38      2340
   macro avg       0.36      0.34      0.34      2340
weighted avg       0.39      0.38      0.37      2340
```





GCN(--seed 40 )

```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.71      0.55      0.62      1005
           1       0.42      0.43      0.43       828
           2       0.26      0.37      0.30       483

    accuracy                           0.47      2316
   macro avg       0.46      0.45      0.45      2316
weighted avg       0.51      0.47      0.49      2316
```

-

```
=========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.45      0.35      0.39      1005
           1       0.43      0.50      0.46       828
           2       0.22      0.27      0.24       483

    accuracy                           0.38      2316
   macro avg       0.37      0.37      0.36      2316
weighted avg       0.39      0.38      0.38      2316

```





# å¯ç”¨GRL+Reconstruction+å¹²é¢„+å¤šé˜¶æ®µè®­ç»ƒç­–ç•¥

## è¯´æ˜



### GRL

```python
class GradReverse(Function):
    @staticmethod
    def forward(ctx, x, lambd=1.0):
        ctx.lambd = lambd
        return x.view_as(x)     # å‰å‘ä¼ æ’­ä¸å˜
    @staticmethod
    def backward(ctx, grad_output):
        return grad_output.neg() * ctx.lambd, None      # åå‘ä¼ æ’­æ—¶ï¼Œæ¥æ”¶åˆ°æ¥è‡ªé¢„æµ‹å™¨çš„æ¢¯åº¦ï¼Œç„¶åå–åå¹¶ä¸”ä¹˜ä¸Šç³»æ•°lambd

def grad_reverse(x, lambd=1.0):
    return GradReverse.apply(x, lambd)

-----------------------------------------------------

self.confound_encoder = MLPEncoder(         # éœ€è¦åŠ å…¥æ­£äº¤æŸå¤±æˆ–è€…å¯¹æŠ—çº¦æŸ
    input_dim=input_dim,
    hidden_dim=hidden_dim,
    output_dim=z_dim,
    num_layers=3,
    dropout=0.1
)

z_c = self.confound_encoder(features) 

# === GRL ===
# è®©z_cæ— æ³•é¢„æµ‹ç—…ç†æ ‡ç­¾
z_c_flat = z_c.mean(dim=(1, 2))     # è¿›è¡Œæ—¶é—´å’Œå…³èŠ‚ç»´åº¦ä¸Šçš„æ± åŒ–
rev_zc=grad_reverse(z_c_flat,lambd=1.0)
confound_logits=self.regressor(rev_zc)		#ä¹Ÿå°±æ˜¯è¯´å’Œz_gåˆ†æ”¯ç”¨çš„æ˜¯åŒä¸€ä¸ªå›å½’å¤´
-----------------------------------------------------

loss = coral_loss(logits, y, num_classes) + F.cross_entropy(outputs["confound_logits"], y)
```



### reconstruction+å¹²é¢„

```python

self.decoder = nn.Sequential(
    nn.Linear(z_dim * 2 , hidden_dim),
    nn.ReLU(),
    nn.Linear(hidden_dim, input_dim)  # é‡æ„å› backbone çš„ feature dim
)

# === ReCon ===
    # é‡æ„æŸå¤±ï¼Œé¿å…é€€åŒ–
    recon_in = torch.cat([z_g,z_c], dim = -1)       #åœ¨Cç»´åº¦ä¸Šè¿›è¡Œæ‹¼æ¥
    recon_features = self.decoder(recon_in)     #è¿›è¡Œdecoderè§£ç 

        
 # === counterfactual ===
    # è®©æ¨¡å‹çŸ¥é“æ ‡ç­¾å¿…é¡»éšç€z_gå˜åŒ–ï¼Œè€Œå¯¹z_cä¿æŒä¸å˜
    # ---- æ„é€ äº¤æ¢æ ·æœ¬ ----
    counterfactual_logits = None
    shuffle_idx=None
    if labels is not None:
        B = z_g_pooled.shape[0]
        shuffle_idx = torch.randperm(B).to(labels.device)	#è¿™é‡Œç”¨çš„æ˜¯éšæœºæ›¿æ¢

        # å°†äº¤æ¢åçš„ç–¾ç—…ç‰¹å¾è¾“å…¥åˆ°å›å½’å¤´è¿›è¡Œé¢„æµ‹
        z_g_swapped = z_g_pooled[shuffle_idx]#<--GRLè¿›è¡Œè§£è€¦
        counterfactual_logits = self.regressor(z_g_swapped)#ç»“æœåº”è¯¥ä¸ç»™å‡ºç—…ç†ç‰¹å¾çš„ä¸ªä½“çš„æ ‡ç­¾å¯¹åº”

```



ç¬¬äºŒé˜¶æ®µï¼š

```python

# è®¡ç®—æ‰€æœ‰æŸå¤±é¡¹
confound_loss = lambd1 * coral_loss(outputs["confound_logits"], y, num_classes)
recon_loss = lambd2 * F.mse_loss(
    outputs["recon_features"].mean(dim=(1, 2)),
    outputs["original_features"].mean(dim=(1, 2))
)

counterfactual_loss = 0
        if outputs["counterfactual_logits"] is not None:
            shuffle_idx = outputs["shuffle_idx"]
            y_swapped = y[shuffle_idx]
            counterfactual_loss = lambd3 * coral_loss(outputs["counterfactual_logits"], y_swapped, num_classes)
```



## å¦‚ä½•éªŒè¯ï¼Ÿ

* adversary loss æ›²çº¿åº”è¯¥éœ‡è¡åœ¨ä¸€ä¸ªæ¯”è¾ƒé«˜çš„ä½ç½®



## ç»“æœï¼ˆå…¨éƒ¨å®‰æ’ä¸Šï¼‰

          0       0.58      0.61      0.60      1026
           1       0.54      0.53      0.53       828
           2       0.37      0.34      0.36       486
    
    accuracy                           0.53      2340
       macro avg       0.50      0.49      0.50      2340
    weighted avg       0.52      0.53      0.52      2340


```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.65      0.73      0.69      1026
           1       0.54      0.44      0.49       828
           2       0.28      0.30      0.29       486

    accuracy                           0.54      2340
   macro avg       0.49      0.49      0.49      2340
weighted avg       0.54      0.54      0.54      2340
```

```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.60      0.70      0.65      1026
           1       0.48      0.37      0.41       828
           2       0.36      0.39      0.37       486

    accuracy                           0.52      2340
   macro avg       0.48      0.48      0.48      2340
weighted avg       0.51      0.52      0.51      2340
```



# 0830ï¼šå‡†å¤‡åšçš„æ”¹åŠ¨è€ƒè™‘

å…ˆæƒè¡¡**ä¸ºå•¥è¦åšï¼Œæœ‰é’ˆå¯¹æ€§**ï¼Œç„¶åè€ƒè™‘**æ˜¯å¦ä¼šæ¶¨ç‚¹**ï¼Œå†è€ƒè™‘**æ€ä¹ˆåš**çš„é—®é¢˜ï¼Œä»¥åŠ**æ€ä¹ˆå®éªŒçœ‹ç»“æœ**çš„é—®é¢˜ï¼Œå‡å°‘é»‘ç®±ï¼Œä¸è¦åªæ˜¯ä¸€å‘³çœ‹æ€»ç»“æœæ˜¯å¦æ¶¨ç‚¹ï¼Œè€Œä¸”ä¸€æ¬¡åšæœ€å°çš„é…å¥—æ”¹åŠ¨å°±å¥½ã€‚



## æ­£äº¤æŸå¤±/HSICï¼Ÿâ‡Œ GRL

* æ˜¯å¦å¯ä»¥å•ç‹¬ç”¨æ­£äº¤æŸå¤±ï¼Ÿ

  



## åŠ å…¥metadataè¿›è¡Œæ˜¾å¼é¢„æµ‹â‡Œ GRLè®©z_cæ— æ³•é¢„æµ‹ç—…ç†æ ‡ç­¾

* GRLï¼šå®è´¨ä¸Šæ˜¯*æ‹”æ²³å¤±è´¥*ï¼Œæ²¡æœ‰æ˜ç¡®çš„å­¦ä¹ ç›®æ ‡ã€‚
* æ˜¾å¼é¢„æµ‹æŒ‡æ ‡ï¼šæ›´æ˜ç¡®ã€å¯å»ºæ¨¡ï¼Œé€¼è¿«åˆ†å·¥



seed 1ï¼šï¼ˆ`loss = coral_loss(logits, y, num_classes) + total_confound_loss + recon_loss`ï¼‰

```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.60      0.52      0.56      1026
           1       0.45      0.46      0.45       828
           2       0.25      0.30      0.27       486

    accuracy                           0.45      2340
   macro avg       0.43      0.43      0.43      2340
weighted avg       0.47      0.45      0.46      2340
```



seed 2:

```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.59      0.52      0.55      1026
           1       0.41      0.42      0.42       828
           2       0.33      0.41      0.37       486

    accuracy                           0.46      2340
   macro avg       0.45      0.45      0.45      2340
weighted avg       0.47      0.46      0.47      2340
```



åŠ å…¥åäº‹å®é€»è¾‘ï¼šseed 2

```
# === ReCon ===
        recon_in = torch.cat([z_g,z_c], dim = -1)       #åœ¨Cç»´åº¦ä¸Šè¿›è¡Œæ‹¼æ¥
        recon_features = self.decoder(recon_in)     #è¿›è¡Œdecoderè§£ç 

        counterfactual_logits=None
        shuffle_idx=None
        if labels is not None:
            # 4. åäº‹å®å¹²é¢„ï¼Œè¿›è¡Œç‰¹å¾äº¤æ¢ï¼Œé€šè¿‡ä¸€è‡´æ€§æŸå¤±è¿›è¡Œçº¦æŸ
            shuffle_idx = get_different_label_shuffled_idx(labels)

            # äº¤æ¢ z_gï¼Œä¿æŒ z_c ä¸å˜
            z_g_shuffled = z_g_pooled[shuffle_idx]

            # ç”¨äº¤æ¢åçš„ z_g è¿›è¡Œé¢„æµ‹
            counterfactual_logits = self.regressor(z_g_shuffled)
```



```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.59      0.52      0.55      1026
           1       0.40      0.41      0.40       828
           2       0.23      0.27      0.25       486

    accuracy                           0.43      2340
   macro avg       0.40      0.40      0.40      2340
weighted avg       0.44      0.43      0.44      2340
```



seed 1

```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.53      0.65      0.58      1026
           1       0.37      0.33      0.35       828
           2       0.24      0.16      0.19       486

    accuracy                           0.44      2340
   macro avg       0.38      0.38      0.37      2340
weighted avg       0.41      0.44      0.42      2340
```







## åŠ¨æ€å­¦ä¹ ç‡&lambd > å›ºå®š

* ğŸ‘‰Uncertainty weightingï¼Œæ¯ä¸ªæŸå¤±é…ä¸€ä¸ªå¯å­¦ä¹ çš„æƒé‡





## åˆ†é˜¶æ®µå†³å®šå¼€lossâ‡Œä¸€å¼€å§‹å°±å…¨å¼€ï¼Œæƒé‡å˜åŒ–â‡Œäº¤æ›¿å°é˜¶æ®µ

* ä¸€å¼€å§‹å°±å…¨å¼€çš„æ–¹æ¡ˆï¼š*sigmoid ramp*
* äº¤æ›¿å°é˜¶æ®µï¼šä¸»ä»»åŠ¡è½»è§£è€¦+è§£è€¦ è½»ä¸»ä»»åŠ¡äº¤æ›¿



### æ‹‰é«˜ç±»åˆ«2æ­£ç¡®ç‡çš„è§£å†³æ–¹æ¡ˆï¼š **class-2 çš„ F1** å¯ä»¥ä½œä¸ºæ—©åœä¿¡å·

ç›´æ¥æŠŠä¼˜åŒ–ç›®æ ‡æŒ‡å‘å…³å¿ƒçš„ç‚¹ï¼Œä½†æ˜¯è¦æ³¨æ„æ˜¯å¦ä¼šå¯¼è‡´å¦å¤–ä¸¤ä¸ªç±»åˆ«ä¸‹é™ã€‚



## éšæœºäº¤æ¢z_gï¼Œç¡®ä¿æ ‡ç­¾åŒç»™å‡ºè€…â‡Œéšæœºäº¤æ¢z_cï¼Œç¡®ä¿æ ‡ç­¾ä¸åŸæ¥ä¸€è‡´

* è¿™äºŒè€…çœŸçš„æœ‰ä¼˜åŠ£ä¹‹åˆ†ï¼Ÿ

* ä½†æ˜¯åœ¨ä»£ç å®ç°å®¹æ˜“/å›°éš¾ç¨‹åº¦ä¸Šï¼Œè¿™æ ·å†™æ— ç–‘è¦æ›´ç®€å•

* ç›¸åº”åœ°æ¢ç”¨ä¸å˜æ€§çº¦æŸï¼š*ä¸ºå•¥è¦åœæ­¢z_gçš„æ¢¯åº¦*ï¼Ÿ

  
