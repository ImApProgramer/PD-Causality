# 0807
## PoseFormerV2
### Baseline
参数：
```
best_params = {
        "lr": 1e-05,
        "num_epochs": 20,
        "num_hidden_layers": 2,
        "layer_sizes": [256, 50, 16, 3],
        "optimizer": 'RMSprop',
        "use_weighted_loss": True,
        "batch_size": 128,
        "dropout_rate": 0.1,
        'weight_decay': 0.00057,
        'momentum': 0.66
    }
```

```
python eval_encoder.py --backbone poseformerv2 --medication 1 --train_mode end2end
```

相比于0801第一次运行的时候没有任何改动。

训练结果如下：
```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.45      0.57      0.50      1005
           1       0.27      0.21      0.24       828
           2       0.30      0.25      0.27       483

    accuracy                           0.37      2316
   macro avg       0.34      0.34      0.34      2316
weighted avg       0.35      0.37      0.36      2316

              precision    recall  f1-score   support

           0       0.62      0.68      0.65       654
           1       0.23      0.21      0.22       306
           2       0.29      0.26      0.27       249

    accuracy                           0.47      1209
   macro avg       0.38      0.38      0.38      1209
weighted avg       0.45      0.47      0.46      1209

              precision    recall  f1-score   support

           0       0.23      0.38      0.29       351
           1       0.30      0.21      0.25       522
           2       0.31      0.24      0.27       234

    accuracy                           0.27      1107
   macro avg       0.28      0.27      0.27      1107
weighted avg       0.28      0.27      0.26      1107

==========LAST REPORTS============
              precision    recall  f1-score   support

           0       0.54      0.52      0.53      1005
           1       0.33      0.30      0.32       828
           2       0.19      0.24      0.21       483

    accuracy                           0.38      2316
   macro avg       0.36      0.35      0.35      2316
weighted avg       0.39      0.38      0.39      2316

              precision    recall  f1-score   support

           0       0.68      0.60      0.64       654
           1       0.34      0.36      0.35       306
           2       0.24      0.29      0.26       249

    accuracy                           0.48      1209
   macro avg       0.42      0.42      0.42      1209
weighted avg       0.50      0.48      0.49      1209

              precision    recall  f1-score   support

           0       0.33      0.36      0.34       351
           1       0.33      0.27      0.29       522
           2       0.14      0.18      0.16       234

    accuracy                           0.28      1107
   macro avg       0.27      0.27      0.27      1107
weighted avg       0.29      0.28      0.28      1107


```

#### 不加入medication
```
python eval_encoder.py --backbone poseformerv2  --train_mode end2end
```

```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.43      0.44      0.44      1005
           1       0.29      0.32      0.30       828
           2       0.15      0.12      0.13       483

    accuracy                           0.33      2316
   macro avg       0.29      0.29      0.29      2316
weighted avg       0.32      0.33      0.33      2316

              precision    recall  f1-score   support

           0       0.61      0.54      0.58       654
           1       0.24      0.30      0.27       306
           2       0.14      0.13      0.13       249

    accuracy                           0.40      1209
   macro avg       0.33      0.33      0.32      1209
weighted avg       0.42      0.40      0.41      1209

              precision    recall  f1-score   support

           0       0.20      0.25      0.22       351
           1       0.32      0.33      0.33       522
           2       0.17      0.11      0.13       234

    accuracy                           0.25      1107
   macro avg       0.23      0.23      0.23      1107
weighted avg       0.25      0.25      0.25      1107

==========LAST REPORTS============
              precision    recall  f1-score   support

           0       0.50      0.62      0.55      1005
           1       0.36      0.29      0.32       828
           2       0.16      0.13      0.14       483

    accuracy                           0.40      2316
   macro avg       0.34      0.35      0.34      2316
weighted avg       0.38      0.40      0.38      2316

              precision    recall  f1-score   support

           0       0.57      0.65      0.61       654
           1       0.25      0.25      0.25       306
           2       0.23      0.16      0.19       249

    accuracy                           0.44      1209
   macro avg       0.35      0.35      0.35      1209
weighted avg       0.42      0.44      0.43      1209

              precision    recall  f1-score   support

           0       0.39      0.56      0.46       351
           1       0.45      0.32      0.37       522
           2       0.11      0.11      0.11       234

    accuracy                           0.35      1107
   macro avg       0.31      0.33      0.31      1107
weighted avg       0.36      0.35      0.34      1107
```

准确率变化不大，但是OFF状态下对于严重类的预测严重下滑，所以这个参数应该保留。这个分支舍弃，返回baseline。

#### 更改epoch到30
`python eval_encoder.py --backbone poseformerv2  --train_mode end2end`

```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.44      0.48      0.46      1005
           1       0.22      0.19      0.20       828
           2       0.18      0.20      0.19       483

    accuracy                           0.32      2316
   macro avg       0.28      0.29      0.28      2316
weighted avg       0.31      0.32      0.31      2316

              precision    recall  f1-score   support

           0       0.59      0.56      0.57       654
           1       0.05      0.05      0.05       306
           2       0.13      0.17      0.15       249

    accuracy                           0.35      1209
   macro avg       0.26      0.26      0.26      1209
weighted avg       0.36      0.35      0.35      1209

              precision    recall  f1-score   support

           0       0.25      0.34      0.29       351
           1       0.33      0.27      0.30       522
           2       0.26      0.23      0.24       234

    accuracy                           0.28      1107
   macro avg       0.28      0.28      0.28      1107
weighted avg       0.29      0.28      0.28      1107

==========LAST REPORTS============
              precision    recall  f1-score   support

           0       0.44      0.61      0.51      1005
           1       0.19      0.14      0.16       828
           2       0.28      0.18      0.22       483

    accuracy                           0.35      2316
   macro avg       0.30      0.31      0.30      2316
weighted avg       0.32      0.35      0.32      2316

              precision    recall  f1-score   support

           0       0.57      0.72      0.64       654
           1       0.13      0.09      0.11       306
           2       0.26      0.17      0.20       249

    accuracy                           0.45      1209
   macro avg       0.32      0.33      0.32      1209
weighted avg       0.39      0.45      0.41      1209

              precision    recall  f1-score   support

           0       0.25      0.43      0.32       351
           1       0.23      0.16      0.19       522
           2       0.31      0.20      0.24       234

    accuracy                           0.25      1107
   macro avg       0.26      0.26      0.25      1107
weighted avg       0.25      0.25      0.24      1107
```

基本上下降，改回epoch=20，分支回退。

#### 更改lr_head为1e-4（为baseline的十倍）

```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.46      0.51      0.48      1005
           1       0.30      0.26      0.28       828
           2       0.22      0.22      0.22       483

    accuracy                           0.36      2316
   macro avg       0.33      0.33      0.33      2316
weighted avg       0.35      0.36      0.36      2316

              precision    recall  f1-score   support

           0       0.61      0.57      0.59       654
           1       0.25      0.26      0.25       306
           2       0.18      0.20      0.19       249

    accuracy                           0.42      1209
   macro avg       0.35      0.34      0.34      1209
weighted avg       0.43      0.42      0.42      1209

              precision    recall  f1-score   support

           0       0.27      0.40      0.32       351
           1       0.35      0.27      0.30       522
           2       0.28      0.24      0.26       234

    accuracy                           0.30      1107
   macro avg       0.30      0.30      0.30      1107
weighted avg       0.31      0.30      0.30      1107

==========LAST REPORTS============
              precision    recall  f1-score   support

           0       0.43      0.46      0.44      1005
           1       0.23      0.21      0.22       828
           2       0.17      0.17      0.17       483

    accuracy                           0.31      2316
   macro avg       0.28      0.28      0.28      2316
weighted avg       0.30      0.31      0.31      2316

              precision    recall  f1-score   support

           0       0.53      0.51      0.52       654
           1       0.08      0.08      0.08       306
           2       0.21      0.23      0.22       249

    accuracy                           0.34      1209
   macro avg       0.27      0.27      0.27      1209
weighted avg       0.35      0.34      0.35      1209

              precision    recall  f1-score   support

           0       0.28      0.36      0.32       351
           1       0.33      0.29      0.31       522
           2       0.12      0.11      0.11       234

    accuracy                           0.27      1107
   macro avg       0.25      0.25      0.25      1107
weighted avg       0.27      0.27      0.27      1107
```

结果并没有更好，回滚baseline。



### ➕optimizer改为AdamW
```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.54      0.51      0.52      1005
           1       0.38      0.43      0.40       828
           2       0.27      0.24      0.25       483

    accuracy                           0.42      2316
   macro avg       0.39      0.39      0.39      2316
weighted avg       0.42      0.42      0.42      2316

              precision    recall  f1-score   support

           0       0.66      0.58      0.62       654
           1       0.34      0.43      0.38       306
           2       0.18      0.18      0.18       249

    accuracy                           0.46      1209
   macro avg       0.39      0.40      0.39      1209
weighted avg       0.48      0.46      0.47      1209

              precision    recall  f1-score   support

           0       0.35      0.38      0.37       351
           1       0.40      0.42      0.41       522
           2       0.38      0.30      0.34       234

    accuracy                           0.38      1107
   macro avg       0.38      0.37      0.37      1107
weighted avg       0.38      0.38      0.38      1107

==========LAST REPORTS============
              precision    recall  f1-score   support

           0       0.51      0.52      0.51      1005
           1       0.37      0.38      0.38       828
           2       0.22      0.20      0.21       483

    accuracy                           0.40      2316
   macro avg       0.37      0.37      0.37      2316
weighted avg       0.40      0.40      0.40      2316

              precision    recall  f1-score   support

           0       0.63      0.58      0.60       654
           1       0.24      0.29      0.26       306
           2       0.10      0.10      0.10       249

    accuracy                           0.41      1209
   macro avg       0.33      0.32      0.32      1209
weighted avg       0.42      0.41      0.41      1209

              precision    recall  f1-score   support

           0       0.33      0.40      0.36       351
           1       0.46      0.44      0.45       522
           2       0.38      0.30      0.34       234

    accuracy                           0.40      1107
   macro avg       0.39      0.38      0.38      1107
weighted avg       0.40      0.40      0.40      1107


```
模型有了不错的提升，认为值得保留，作为新的baseline。

## CTR-GCN


### Baseline

* 没做数据增强
* 用的是CE loss
* 参数如下：
  ```
  best_params = {     # ⚠️这些参数是否合理呢？
            "lr": 0.1,    #0807调参：似乎有点太大了，从原来的0.1调整到0.001
            "num_epochs": 20,
            "batch_size": 64,
            "optimizer": 'SGD',
            "weight_decay": 0.0001,
            "momentum": 0.9,
            "dropout_rate": 0.3,  # 0807调参：保持和下面一致
            "use_weighted_loss": False  # 0807调参修改为True
        }
  ```

结果：


```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.55      0.53      0.54      1005
           1       0.36      0.32      0.34       828
           2       0.14      0.17      0.16       483
    
    accuracy                           0.38      2316
   macro avg       0.35      0.34      0.35      2316
weighted avg       0.40      0.38      0.39      2316

              precision    recall  f1-score   support
    
           0       0.67      0.55      0.60       654
           1       0.23      0.25      0.24       306
           2       0.07      0.10      0.08       249
    
    accuracy                           0.38      1209
   macro avg       0.32      0.30      0.31      1209
weighted avg       0.43      0.38      0.40      1209

              precision    recall  f1-score   support
    
           0       0.40      0.50      0.45       351
           1       0.47      0.36      0.41       522
           2       0.23      0.26      0.24       234
    
    accuracy                           0.38      1107
   macro avg       0.37      0.37      0.36      1107
weighted avg       0.40      0.38      0.38      1107

==========LAST REPORTS============
              precision    recall  f1-score   support

           0       0.55      0.67      0.60      1005
           1       0.38      0.29      0.33       828
           2       0.22      0.19      0.21       483
    
    accuracy                           0.44      2316
   macro avg       0.38      0.39      0.38      2316
weighted avg       0.42      0.44      0.42      2316

              precision    recall  f1-score   support
    
           0       0.60      0.66      0.63       654
           1       0.25      0.19      0.21       306
           2       0.01      0.01      0.01       249
    
    accuracy                           0.41      1209
   macro avg       0.29      0.29      0.28      1209
weighted avg       0.39      0.41      0.40      1209

              precision    recall  f1-score   support
    
           0       0.47      0.70      0.56       351
           1       0.45      0.36      0.40       522
           2       0.55      0.39      0.45       234
    
    accuracy                           0.47      1107
   macro avg       0.49      0.48      0.47      1107
weighted avg       0.47      0.47      0.46      1107

```



### ➕数据增强

在数据增强中，各项一起启用，显著改善了训练过程中的过拟合，现在基本每个fold的train_accuracy都不到0.6。
如：
```
Training (fold1):   0%| | 0/20 [00:06<?, ?epoch/s, train_accuracy=40.5, train_lo[INFO] Best checkpoint saved at epoch 0 with val_f1_score=0.1319
Training (fold1):   5%| | 1/20 [00:06<02:08,  6.78s/epoch, train_accuracy=40.5, Epoch 1 completed in 5.92s
Training (fold1):  10%| | 2/20 [00:13<02:01,  6.75s/epoch, train_accuracy=46.2, Epoch 2 completed in 5.90s
Training (fold1):  10%| | 2/20 [00:20<02:01,  6.75s/epoch, train_accuracy=44.2, [INFO] Best checkpoint saved at epoch 2 with val_f1_score=0.4785
Training (fold1):  15%|▏| 3/20 [00:20<01:54,  6.76s/epoch, train_accuracy=44.2, Epoch 3 completed in 5.93s
Training (fold1):  15%|▏| 3/20 [00:27<01:54,  6.76s/epoch, train_accuracy=49.1, [INFO] Best checkpoint saved at epoch 3 with val_f1_score=0.5810
Training (fold1):  20%|▏| 4/20 [00:27<01:48,  6.79s/epoch, train_accuracy=49.1, Epoch 4 completed in 5.90s
Training (fold1):  20%|▏| 4/20 [00:33<01:48,  6.79s/epoch, train_accuracy=49, tr[INFO] Best checkpoint saved at epoch 4 with val_f1_score=0.6231
Training (fold1):  25%|▎| 5/20 [00:33<01:41,  6.79s/epoch, train_accuracy=49, trEpoch 5 completed in 5.94s
Training (fold1):  30%|▎| 6/20 [00:40<01:35,  6.80s/epoch, train_accuracy=48.8, Epoch 6 completed in 5.94s
Training (fold1):  35%|▎| 7/20 [00:47<01:28,  6.78s/epoch, train_accuracy=50.4, Epoch 7 completed in 5.92s
Training (fold1):  40%|▍| 8/20 [00:54<01:21,  6.76s/epoch, train_accuracy=48.6, Epoch 8 completed in 5.93s
Training (fold1):  45%|▍| 9/20 [01:00<01:14,  6.76s/epoch, train_accuracy=56, trEpoch 9 completed in 5.94s
Training (fold1):  50%|▌| 10/20 [01:07<01:07,  6.75s/epoch, train_accuracy=48.7,Epoch 10 completed in 5.94s
Training (fold1):  55%|▌| 11/20 [01:14<01:00,  6.75s/epoch, train_accuracy=55.7,Epoch 11 completed in 5.93s
Training (fold1):  60%|▌| 12/20 [01:21<00:53,  6.75s/epoch, train_accuracy=52.8,Epoch 12 completed in 5.92s
Training (fold1):  65%|▋| 13/20 [01:27<00:47,  6.74s/epoch, train_accuracy=56.9,Epoch 13 completed in 5.92s
Training (fold1):  70%|▋| 14/20 [01:34<00:40,  6.74s/epoch, train_accuracy=53.2,Epoch 14 completed in 5.94s
Training (fold1):  75%|▊| 15/20 [01:41<00:33,  6.74s/epoch, train_accuracy=54.3,Epoch 15 completed in 5.94s
Training (fold1):  75%|▊| 15/20 [01:48<00:33,  6.74s/epoch, train_accuracy=58.9,[INFO] Best checkpoint saved at epoch 15 with val_f1_score=0.6267
Training (fold1):  80%|▊| 16/20 [01:48<00:27,  6.87s/epoch, train_accuracy=58.9,Epoch 16 completed in 5.93s
Training (fold1):  85%|▊| 17/20 [01:55<00:20,  6.83s/epoch, train_accuracy=56.4,Epoch 17 completed in 5.94s
Training (fold1):  90%|▉| 18/20 [02:02<00:13,  6.81s/epoch, train_accuracy=57.9,Epoch 18 completed in 5.93s
Training (fold1):  95%|▉| 19/20 [02:08<00:06,  6.79s/epoch, train_accuracy=58.6,Epoch 19 completed in 5.92s
Training (fold1): 100%|█| 20/20 [02:15<00:00,  6.77s/epoch, train_accuracy=55.9,
```



#### 结果

提升依然不是很大，f1-score accuracy涨了0.03左右。

```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.55      0.55      0.55      1005
           1       0.36      0.31      0.33       828
           2       0.23      0.29      0.26       483

    accuracy                           0.41      2316
   macro avg       0.38      0.38      0.38      2316
weighted avg       0.42      0.41      0.41      2316

              precision    recall  f1-score   support

           0       0.55      0.62      0.58       654
           1       0.30      0.23      0.26       306
           2       0.16      0.16      0.16       249

    accuracy                           0.43      1209
   macro avg       0.34      0.34      0.34      1209
weighted avg       0.41      0.43      0.41      1209

              precision    recall  f1-score   support

           0       0.54      0.41      0.47       351
           1       0.39      0.36      0.37       522
           2       0.28      0.44      0.34       234

    accuracy                           0.39      1107
   macro avg       0.40      0.40      0.39      1107
weighted avg       0.42      0.39      0.40      1107

==========LAST REPORTS============
              precision    recall  f1-score   support

           0       0.48      0.43      0.45      1005
           1       0.35      0.48      0.40       828
           2       0.37      0.21      0.27       483

    accuracy                           0.40      2316
   macro avg       0.40      0.37      0.38      2316
weighted avg       0.41      0.40      0.40      2316

              precision    recall  f1-score   support

           0       0.53      0.56      0.54       654
           1       0.24      0.30      0.27       306
           2       0.52      0.25      0.34       249

    accuracy                           0.43      1209
   macro avg       0.43      0.37      0.38      1209
weighted avg       0.45      0.43      0.43      1209

              precision    recall  f1-score   support

           0       0.31      0.19      0.24       351
           1       0.41      0.58      0.48       522
           2       0.26      0.17      0.20       234

    accuracy                           0.37      1107
   macro avg       0.33      0.31      0.31      1107
weighted avg       0.35      0.37      0.34      1107
```



### ➕使用Weighted Loss

这一步其实也是必须做的。



#### 结果

差不多涨了0.03。

```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.51      0.68      0.58      1005
           1       0.37      0.18      0.24       828
           2       0.32      0.37      0.34       483

    accuracy                           0.44      2316
   macro avg       0.40      0.41      0.39      2316
weighted avg       0.42      0.44      0.41      2316

              precision    recall  f1-score   support

           0       0.54      0.69      0.60       654
           1       0.00      0.00      0.00       306
           2       0.20      0.17      0.18       249

    accuracy                           0.41      1209
   macro avg       0.24      0.29      0.26      1209
weighted avg       0.33      0.41      0.36      1209

              precision    recall  f1-score   support

           0       0.47      0.68      0.55       351
           1       0.60      0.29      0.39       522
           2       0.39      0.59      0.47       234

    accuracy                           0.47      1107
   macro avg       0.49      0.52      0.47      1107
weighted avg       0.52      0.47      0.46      1107

==========LAST REPORTS============
              precision    recall  f1-score   support

           0       0.60      0.53      0.56      1005
           1       0.36      0.43      0.39       828
           2       0.31      0.28      0.29       483

    accuracy                           0.44      2316
   macro avg       0.42      0.41      0.42      2316
weighted avg       0.45      0.44      0.44      2316

              precision    recall  f1-score   support

           0       0.60      0.60      0.60       654
           1       0.10      0.12      0.11       306
           2       0.23      0.18      0.20       249

    accuracy                           0.39      1209
   macro avg       0.31      0.30      0.30      1209
weighted avg       0.40      0.39      0.39      1209

              precision    recall  f1-score   support

           0       0.61      0.39      0.48       351
           1       0.50      0.61      0.55       522
           2       0.38      0.38      0.38       234

    accuracy                           0.50      1107
   macro avg       0.50      0.46      0.47      1107
weighted avg       0.51      0.50      0.49      1107
```





### ➕参数调整，全部套用默认的+128 batch size

```
        best_params = {     # ⚠️这些参数是否合理呢？
            "lr": 1e-05,    #原本0.1
            "num_epochs": 20,
            "batch_size": 128,	#原本64
            "optimizer": 'AdamW',	#原本SGD
            "weight_decay": 0.00057,	#原本0.0001
            "momentum": 0.66,			#原本0.9
            "dropout_rate": 0.1,  		#原本0.3
            "use_weighted_loss": True  # 0807调参修改为True
        }
```

直接豁出去了。

结果过拟合似乎再次出现：

```
Training (fold0):  75%|▊| 15/20 [01:38<00:32,  6.50s/epoch, train_accuracy=86, tEpoch 15 completed in 5.50s
Training (fold0):  80%|▊| 16/20 [01:45<00:25,  6.47s/epoch, train_accuracy=87.5,Epoch 16 completed in 5.50s
Training (fold0):  85%|▊| 17/20 [01:51<00:19,  6.46s/epoch, train_accuracy=87.7,Epoch 17 completed in 5.48s
Training (fold0):  85%|▊| 17/20 [01:58<00:19,  6.46s/epoch, train_accuracy=88.9,[INFO] Best checkpoint saved at epoch 17 with val_f1_score=0.4461
Training (fold0):  90%|▉| 18/20 [01:58<00:12,  6.48s/epoch, train_accuracy=88.9,Epoch 18 completed in 5.50s
Training (fold0):  95%|▉| 19/20 [02:04<00:06,  6.47s/epoch, train_accuracy=87.9,Epoch 19 completed in 5.49s
Training (fold0):  95%|▉| 19/20 [02:11<00:06,  6.47s/epoch, train_accuracy=89.8,[INFO] Best checkpoint saved at epoch 19 with val_f1_score=0.4528
Training (fold0): 100%|█| 20/20 [02:11<00:00,  6.57s/epoch, train_accuracy=89.8,
```



训练结果尚可！

```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.62      0.70      0.66      1005
           1       0.42      0.47      0.44       828
           2       0.22      0.11      0.15       483

    accuracy                           0.50      2316
   macro avg       0.42      0.43      0.42      2316
weighted avg       0.46      0.50      0.48      2316

              precision    recall  f1-score   support

           0       0.68      0.70      0.69       654
           1       0.39      0.57      0.46       306
           2       0.10      0.04      0.06       249

    accuracy                           0.53      1209
   macro avg       0.39      0.43      0.40      1209
weighted avg       0.49      0.53      0.50      1209

              precision    recall  f1-score   support

           0       0.52      0.70      0.60       351
           1       0.44      0.42      0.43       522
           2       0.31      0.19      0.24       234

    accuracy                           0.46      1107
   macro avg       0.42      0.44      0.42      1107
weighted avg       0.44      0.46      0.44      1107

==========LAST REPORTS============
              precision    recall  f1-score   support

           0       0.65      0.78      0.71      1005
           1       0.55      0.57      0.56       828
           2       0.40      0.22      0.29       483

    accuracy                           0.59      2316
   macro avg       0.53      0.52      0.52      2316
weighted avg       0.56      0.59      0.57      2316

              precision    recall  f1-score   support

           0       0.69      0.74      0.71       654
           1       0.48      0.56      0.52       306
           2       0.34      0.20      0.26       249

    accuracy                           0.58      1209
   macro avg       0.50      0.50      0.50      1209
weighted avg       0.56      0.58      0.57      1209

              precision    recall  f1-score   support

           0       0.60      0.84      0.70       351
           1       0.60      0.57      0.58       522
           2       0.48      0.24      0.32       234

    accuracy                           0.59      1107
   macro avg       0.56      0.55      0.53      1107
weighted avg       0.57      0.59      0.56      1107
```



接下来可以考虑改一下optimizer到原本的那个，但是在那之前可以再跑一波，确认不是震荡。

又跑了一波，总的来说，很稳：

```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.61      0.69      0.65      1005
           1       0.41      0.45      0.43       828
           2       0.24      0.14      0.17       483

    accuracy                           0.49      2316
   macro avg       0.42      0.42      0.42      2316
weighted avg       0.46      0.49      0.47      2316

              precision    recall  f1-score   support

           0       0.68      0.69      0.68       654
           1       0.41      0.57      0.47       306
           2       0.18      0.09      0.12       249

    accuracy                           0.53      1209
   macro avg       0.42      0.45      0.43      1209
weighted avg       0.51      0.53      0.51      1209

              precision    recall  f1-score   support

           0       0.51      0.69      0.58       351
           1       0.42      0.38      0.39       522
           2       0.28      0.19      0.23       234

    accuracy                           0.44      1107
   macro avg       0.40      0.42      0.40      1107
weighted avg       0.42      0.44      0.42      1107

==========LAST REPORTS============
              precision    recall  f1-score   support

           0       0.65      0.78      0.71      1005
           1       0.55      0.57      0.56       828
           2       0.41      0.22      0.29       483

    accuracy                           0.59      2316
   macro avg       0.54      0.52      0.52      2316
weighted avg       0.56      0.59      0.57      2316

              precision    recall  f1-score   support

           0       0.68      0.74      0.71       654
           1       0.48      0.55      0.51       306
           2       0.35      0.20      0.26       249

    accuracy                           0.58      1209
   macro avg       0.50      0.50      0.50      1209
weighted avg       0.56      0.58      0.57      1209

              precision    recall  f1-score   support

           0       0.60      0.83      0.70       351
           1       0.60      0.57      0.58       522
           2       0.48      0.24      0.32       234

    accuracy                           0.59      1107
   macro avg       0.56      0.55      0.54      1107
weighted avg       0.57      0.59      0.57      1107
```



```
==========BEST REPORTS============
              precision    recall  f1-score   support
           0       0.70      0.69      0.70      1005
           1       0.43      0.43      0.43       828
           2       0.10      0.10      0.10       483
    accuracy                           0.48      2316
   macro avg       0.41      0.41      0.41      2316
weighted avg       0.48      0.48      0.48      2316
              precision    recall  f1-score   support
           0       0.71      0.72      0.72       654
           1       0.34      0.44      0.38       306
           2       0.00      0.00      0.00       249
    accuracy                           0.50      1209
   macro avg       0.35      0.39      0.37      1209
weighted avg       0.47      0.50      0.49      1209
              precision    recall  f1-score   support
           0       0.68      0.64      0.66       351
           1       0.50      0.43      0.46       522
           2       0.15      0.21      0.17       234
    accuracy                           0.45      1107
   macro avg       0.44      0.43      0.43      1107
weighted avg       0.48      0.45      0.46      1107
==========LAST REPORTS============
              precision    recall  f1-score   support
           0       0.65      0.69      0.67      1005
           1       0.42      0.46      0.44       828
           2       0.17      0.12      0.14       483
    accuracy                           0.49      2316
   macro avg       0.41      0.42      0.41      2316
weighted avg       0.47      0.49      0.47      2316
              precision    recall  f1-score   support
           0       0.70      0.64      0.67       654
           1       0.33      0.48      0.39       306
           2       0.09      0.06      0.07       249
    accuracy                           0.48      1209
   macro avg       0.37      0.39      0.38      1209
weighted avg       0.48      0.48      0.48      1209
              precision    recall  f1-score   support
           0       0.58      0.77      0.66       351
           1       0.50      0.45      0.47       522
           2       0.25      0.18      0.21       234
    accuracy                           0.49      1107
   macro avg       0.44      0.47      0.45      1107
weighted avg       0.47      0.49      0.48      1107
```



#### ➕使用LR Scheduler 

目前的参数参考其他backbone：

```
'scheduler': "StepLR",
'lr_step_size': 1,
'lr_decay': 0.99
```





结果又稍微降了一些：

```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.69      0.70      0.70      1005
           1       0.41      0.41      0.41       828
           2       0.10      0.10      0.10       483

    accuracy                           0.47      2316
   macro avg       0.40      0.40      0.40      2316
weighted avg       0.47      0.47      0.47      2316

              precision    recall  f1-score   support

           0       0.73      0.74      0.74       654
           1       0.36      0.45      0.40       306
           2       0.00      0.00      0.00       249

    accuracy                           0.52      1209
   macro avg       0.36      0.40      0.38      1209
weighted avg       0.48      0.52      0.50      1209

              precision    recall  f1-score   support

           0       0.62      0.63      0.62       351
           1       0.46      0.38      0.41       522
           2       0.15      0.21      0.17       234

    accuracy                           0.42      1107
   macro avg       0.41      0.40      0.40      1107
weighted avg       0.45      0.42      0.43      1107

==========LAST REPORTS============
              precision    recall  f1-score   support

           0       0.65      0.69      0.67      1005
           1       0.41      0.46      0.43       828
           2       0.15      0.11      0.12       483

    accuracy                           0.48      2316
   macro avg       0.40      0.42      0.41      2316
weighted avg       0.46      0.48      0.47      2316

              precision    recall  f1-score   support

           0       0.70      0.64      0.67       654
           1       0.33      0.49      0.39       306
           2       0.04      0.02      0.03       249

    accuracy                           0.47      1209
   macro avg       0.36      0.38      0.36      1209
weighted avg       0.47      0.47      0.47      1209

              precision    recall  f1-score   support

           0       0.58      0.78      0.67       351
           1       0.49      0.44      0.46       522
           2       0.25      0.19      0.22       234

    accuracy                           0.49      1107
   macro avg       0.44      0.47      0.45      1107
weighted avg       0.47      0.49      0.48      1107

```



## motionagformer

值得强调的是，到目前为止没有任何除了CTR-GCN之外的模型解决过过拟合问题，尤其是motionagformer，每个fold跑完20个epoch都会达到100的train_accuracy。



### batch=4

##### 没加medication信息

参数（⚠寄，batch_size过小，得改成32再跑一遍）

```
    elif backbone_name == 'motionagformer':
        best_params = {
            "lr": 1e-05,
            "num_epochs": 20,
            "num_hidden_layers": 2,
            "layer_sizes": [256, 50, 16, 3],
            "optimizer": 'RMSprop',
            "use_weighted_loss": True,
            "batch_size": 4,
            "dropout_rate": 0.1,
            'weight_decay': 0.00057,
            'momentum': 0.66
        }
```





```

==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.56      0.69      0.62      1026
           1       0.50      0.48      0.49       828
           2       0.51      0.28      0.37       486

    accuracy                           0.53      2340
   macro avg       0.52      0.49      0.49      2340
weighted avg       0.53      0.53      0.52      2340

              precision    recall  f1-score   support

           0       0.69      0.72      0.71       675
           1       0.42      0.53      0.46       306
           2       0.48      0.29      0.36       252

    accuracy                           0.58      1233
   macro avg       0.53      0.51      0.51      1233
weighted avg       0.58      0.58      0.57      1233

              precision    recall  f1-score   support

           0       0.40      0.65      0.50       351
           1       0.57      0.46      0.51       522
           2       0.56      0.28      0.38       234

    accuracy                           0.48      1107
   macro avg       0.51      0.46      0.46      1107
weighted avg       0.51      0.48      0.48      1107

==========LAST REPORTS============
              precision    recall  f1-score   support

           0       0.50      0.63      0.56      1026
           1       0.54      0.47      0.50       828
           2       0.45      0.30      0.36       486

    accuracy                           0.51      2340
   macro avg       0.50      0.47      0.48      2340
weighted avg       0.51      0.51      0.50      2340

              precision    recall  f1-score   support

           0       0.57      0.61      0.59       675
           1       0.32      0.35      0.34       306
           2       0.38      0.25      0.30       252

    accuracy                           0.47      1233
   macro avg       0.42      0.41      0.41      1233
weighted avg       0.47      0.47      0.47      1233

              precision    recall  f1-score   support

           0       0.42      0.68      0.52       351
           1       0.74      0.53      0.62       522
           2       0.54      0.36      0.43       234

    accuracy                           0.54      1107
   macro avg       0.57      0.53      0.52      1107
weighted avg       0.60      0.54      0.55      1107

/opt/conda/envs/1128/lib/python3.8/site-packages/scipy/stats/_morestats.py:3414: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.
  warnings.warn("Exact p-value calculation does not work if there are "
/opt/conda/envs/1128/lib/python3.8/site-packages/scipy/stats/_morestats.py:3414: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.
  warnings.warn("Exact p-value calculation does not work if there are "
wandb:                                                                                
wandb: 
wandb: Run history:
wandb:                          epoch ██▁▁▇▇▃▅█▂██▃▆▇▆▆█▂▃▁▆▆▄█▃▆▆▁▅▆█▂▆▆▃▅▂▃▇
wandb:        eval_acc/fold0_accuracy ▄▇▇█▇▇█▇▅▆▅▇▁▇█▇▆▇▆▇
wandb:       eval_acc/fold10_accuracy ▂▁▅▂▆▂▇▆▆▃▂▃▂▃▃▄█▄▁▃
wandb:       eval_acc/fold11_accuracy █▇▅▅▄▇▃▄▄▂▄▄▅▄▃▃▁▄▃▃
wandb:       eval_acc/fold12_accuracy █▆▆▄▄▅▂▄▁▆▄▃▅▄▃▂▂▃▄▃
wandb:       eval_acc/fold13_accuracy ▄▃▂▃▃▂▂▄▄█▃▄▄▃▂▁▄▅▃▄
wandb:       eval_acc/fold14_accuracy ▅█▆▁▄▇█▅▆█▅▇▄▆▄▁▅▃▃▄
wandb:       eval_acc/fold15_accuracy ▄▃▃▃▂▄▆▃█▆▆▅▇▇▇▁▃▆▅▄
wandb:       eval_acc/fold16_accuracy ▇█▃▄▁▃▁▃▂▅▃▂▁▁▃▁▁▇▁▁
wandb:       eval_acc/fold17_accuracy ▁█▄▄▄▅█▆▆▅▃█▅▅▆▇▁▅▂▅
wandb:       eval_acc/fold18_accuracy ▅█▁▇▇▇█▇▆█▇▇▆▇▇▇██▆▇
wandb:       eval_acc/fold19_accuracy ▃▄▁▆▄▂▄▆███▇▇▆▇▃▅▄▃▂
wandb:        eval_acc/fold1_accuracy ▁▄▇▆▇▆▇▄▆▄▇▅▅▆▅▆█▄▇▆
wandb:       eval_acc/fold20_accuracy ▂▄▄▇▆▇▆█▆▄▅▅▅▃▃▄▂▁▄▄
wandb:       eval_acc/fold21_accuracy ▆▇▂▅▆▆▄▃▁▄▂▅▆▅▆▄▅▅█▃
wandb:       eval_acc/fold22_accuracy ▇▄▄▂▂▃▃▃▁▂▂▄▂▁▅█▃▁▃▁
wandb:        eval_acc/fold2_accuracy ▇██▆▇█▁▆▄▅▁▄▂▅▄▆▇▄▅▆
wandb:        eval_acc/fold3_accuracy ████████▄▆█▆▆█▆▇▁██▇
wandb:        eval_acc/fold4_accuracy █▅▆▇▇▆▄▇▆▇▇██▇▅▅▁▆▆▅
wandb:        eval_acc/fold5_accuracy ▆▇▇▁▄▅▄▄▃▃█▅▄▄▅▅▃▇▆▃
wandb:        eval_acc/fold6_accuracy ▆▇█▆▅▃▁▅▂▃▃▂▁▃▂▂▂▁▁▂
wandb:        eval_acc/fold7_accuracy ▇▇▃▅▅▃▂▃█▄▂▁▇▃▂▂▁▂▂▁
wandb:        eval_acc/fold8_accuracy ▇███▄▆▁▄▅▃▃▆▄▂▃▆▄▁▃▄
wandb:        eval_acc/fold9_accuracy █▂▂▂▃▃▅▄▂▃▃▂▃▂▂▂▂▂▃▁
wandb:               eval_f1/fold0_f1 ▄▆██▇▆▇▇▅▆▅▇▁▇▇▇▅█▆▇
wandb:              eval_f1/fold10_f1 ▂▁▅▃▆▂▆▅▆▃▂▃▂▃▃▅█▄▂▃
wandb:              eval_f1/fold11_f1 █▆▆▆▅▇▄▄▄▂▄▄▅▅▄▄▁▄▄▄
wandb:              eval_f1/fold12_f1 █▅▅▅▄▄▂▄▁▆▃▄▄▄▃▃▂▄▄▃
wandb:              eval_f1/fold13_f1 ▄▃▂▂▂▂▂▄▃█▃▄▄▃▂▁▄▄▃▃
wandb:              eval_f1/fold14_f1 ▄█▆▂▅▆█▅▅█▅▆▄▆▃▁▅▂▃▃
wandb:              eval_f1/fold15_f1 ▃▂▂▃▂▃▅▁█▅▆▄▇▆▇▁▂▆▄▄
wandb:              eval_f1/fold16_f1 ██▄▄▁▄▁▃▂▄▃▂▂▂▄▂▂▇▁▁
wandb:              eval_f1/fold17_f1 ▁▇▅▄▅▅▇▆▆▄▃█▅▅▆▇▁▄▂▅
wandb:              eval_f1/fold18_f1 ▆█▁▇▇▇█▇▆▇█▇▆▇▇▇██▆█
wandb:              eval_f1/fold19_f1 ▄▃▁▆▅▂▅▆███▇▆▇▇▃▆▅▄▃
wandb:               eval_f1/fold1_f1 ▁▃▇▆▆▆▇▄▆▄▇▅▄▅▅▆█▄▇▅
wandb:              eval_f1/fold20_f1 ▁▄▅▇▅▇▆█▇▄▅▃▅▄▃▄▄▂▄▄
wandb:              eval_f1/fold21_f1 ▅█▂▆▆▅▄▃▁▅▂▃▇▅▅▃▄▄█▃
wandb:              eval_f1/fold22_f1 █▄▄▃▂▄▃▃▁▂▃▄▃▁▄█▂▁▃▂
wandb:               eval_f1/fold2_f1 ▄██▆▆█▁▆▄▅▂▄▂▅▅▅▆▅▅▆
wandb:               eval_f1/fold3_f1 █▇▇▇▇▇▇▇▃▆▇▅▅▇▅▆▁▇▇▆
wandb:               eval_f1/fold4_f1 █▅▅▅▆▅▄▆▇▅▇▇▆▅▅▄▁▆▅▄
wandb:               eval_f1/fold5_f1 ▆▇▇▂▅▆▅▅▄▁█▆▅▆▆▆▅▇▆▄
wandb:               eval_f1/fold6_f1 ▇▆█▇▄▄▁▅▁▃▃▂▂▄▁▁▃▁▁▁
wandb:               eval_f1/fold7_f1 ▅▇▄▅▆▃▂▃█▅▁▁▇▃▂▃▁▂▂▁
wandb:               eval_f1/fold8_f1 ▆███▄▇▃▅▅▂▄▇▄▂▂▄▅▁▄▃
wandb:               eval_f1/fold9_f1 █▁▂▁▃▂▅▄▂▂▂▂▂▂▂▂▂▂▂▁
wandb:           eval_loss/fold0_loss ▁▂▁▂▄▄▃▃▅▅▃▃█▅▃▄▄▃▅▃
wandb:          eval_loss/fold10_loss ▁▃▅▆▅▆▆▆▇▇▇▇█▇█▇▆▆█▇
wandb:          eval_loss/fold11_loss ▁▂▃▄▅▃▆▅▄▆▅▅▅▆▆▇█▅▆▇
wandb:          eval_loss/fold12_loss ▁▂▃▅▅▅█▅█▂▆▆▅▆▆▆▇▇▇█
wandb:          eval_loss/fold13_loss ▂▂▄▅▅▆▅▅▅▁▅▄▄▅██▅▃▆▅
wandb:          eval_loss/fold14_loss ▁▁▃▅▄▃▂▄▄▂▅▄▅▃▆█▅▇▆▇
wandb:          eval_loss/fold15_loss ▁▃▅▅▆▄▂▅▂▃▃▄▃▃▃█▅▄▅▅
wandb:          eval_loss/fold16_loss ▁▂▃▄▇▆▅▅▆▅▆▇▆▆▅▆▆▄██
wandb:          eval_loss/fold17_loss ▁▁▅▄▄▄▃▃▄▃▇▃▇▄▃▂█▄▆▅
wandb:          eval_loss/fold18_loss ▂▁█▂▃▃▃▃▄▃▃▄▄▄▄▃▃▃▅▃
wandb:          eval_loss/fold19_loss ▂▃▆▄▅▆▁▁▂▂▂▄▄▃▄█▅▆▆▄
wandb:           eval_loss/fold1_loss ▆▅▂▄▅▃▂▆▃▆▂▃▅▅▄▃▁█▃▄
wandb:          eval_loss/fold20_loss ▁▂▃▄▂▂▃▃▃▄▃▅▅▆▆▅▇█▇█
wandb:          eval_loss/fold21_loss ▁▁▄▃▄▄▅▄▆▅▅▆▆▅▅▆▆▆▅█
wandb:          eval_loss/fold22_loss ▁▃▅▆▆▅▇▆▆▆▇▇▆▇▆▆▇█▆▇
wandb:           eval_loss/fold2_loss ▁▁▂▄▅▅▇▆▆▇█▆█▆█▇▆▇▇▆
wandb:           eval_loss/fold3_loss ▁▂▂▃▃▄▃▄▆▄▃▆▆▄▅▅█▅▅▅
wandb:           eval_loss/fold4_loss ▁▄▄▅▅▅▅▄▄▆▄▅▄▅▆▇▇▅▆█
wandb:           eval_loss/fold5_loss ▁▁▂▇▅▃▅▅▅█▂▄▄▅▄▅▅▃▄▆
wandb:           eval_loss/fold6_loss ▁▃▄▅▆▅▇▆▆▆▆▆▇▆▇▇▇███
wandb:           eval_loss/fold7_loss ▁▁▃▃▄▄▄▅▄▄██▅▇█▆▆▆▅▇
wandb:           eval_loss/fold8_loss ▁▁▂▄▆▄▆▄▅▇▅▅▆▆▇▄▃█▆▆
wandb:           eval_loss/fold9_loss ▁▃▃▅▃▅▆▅▆▅▆▇▆▇▇█▇█▇▇
wandb:                 train/fold0_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                train/fold10_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                train/fold11_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                train/fold12_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                train/fold13_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                train/fold14_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                train/fold15_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                train/fold16_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                train/fold17_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                train/fold18_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                train/fold19_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                 train/fold1_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                train/fold20_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                train/fold21_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                train/fold22_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                 train/fold2_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                 train/fold3_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                 train/fold4_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                 train/fold5_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                 train/fold6_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                 train/fold7_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                 train/fold8_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:                 train/fold9_lr ██▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▂▁▁
wandb:  train_accuracy/fold0_accuracy ▁▇██████████████████
wandb: train_accuracy/fold10_accuracy ▁▆██████████████████
wandb: train_accuracy/fold11_accuracy ▁▇██████████████████
wandb: train_accuracy/fold12_accuracy ▁▇██████████████████
wandb: train_accuracy/fold13_accuracy ▁▆▇█████████████████
wandb: train_accuracy/fold14_accuracy ▁▇██████████████████
wandb: train_accuracy/fold15_accuracy ▁▇██████████████████
wandb: train_accuracy/fold16_accuracy ▁▆██████████████████
wandb: train_accuracy/fold17_accuracy ▁▇██████████████████
wandb: train_accuracy/fold18_accuracy ▁▇██████████████████
wandb: train_accuracy/fold19_accuracy ▁▆██████████████████
wandb:  train_accuracy/fold1_accuracy ▁▆██████████████████
wandb: train_accuracy/fold20_accuracy ▁▆██████████████████
wandb: train_accuracy/fold21_accuracy ▁▇██████████████████
wandb: train_accuracy/fold22_accuracy ▁▇██████████████████
wandb:  train_accuracy/fold2_accuracy ▁▆██████████████████
wandb:  train_accuracy/fold3_accuracy ▁▇██████████████████
wandb:  train_accuracy/fold4_accuracy ▁▇██████████████████
wandb:  train_accuracy/fold5_accuracy ▁▆██████████████████
wandb:  train_accuracy/fold6_accuracy ▁▅██████████████████
wandb:  train_accuracy/fold7_accuracy ▁▅██████████████████
wandb:  train_accuracy/fold8_accuracy ▁▃▇█████████████████
wandb:  train_accuracy/fold9_accuracy ▁▆██████████████████
wandb:          train_loss/fold0_loss █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train_loss/fold10_loss █▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train_loss/fold11_loss █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train_loss/fold12_loss █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train_loss/fold13_loss █▅▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train_loss/fold14_loss █▅▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train_loss/fold15_loss █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train_loss/fold16_loss █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train_loss/fold17_loss █▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train_loss/fold18_loss █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train_loss/fold19_loss █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train_loss/fold1_loss █▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train_loss/fold20_loss █▅▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train_loss/fold21_loss █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:         train_loss/fold22_loss █▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train_loss/fold2_loss █▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train_loss/fold3_loss █▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train_loss/fold4_loss █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train_loss/fold5_loss █▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train_loss/fold6_loss █▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train_loss/fold7_loss █▆▃▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train_loss/fold8_loss █▆▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:          train_loss/fold9_loss █▄▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb:                          epoch 19
wandb:        eval_acc/fold0_accuracy 78.16092
wandb:       eval_acc/fold10_accuracy 31.30755
wandb:       eval_acc/fold11_accuracy 34.24908
wandb:       eval_acc/fold12_accuracy 40.17467
wandb:       eval_acc/fold13_accuracy 66.9697
wandb:       eval_acc/fold14_accuracy 64.37768
wandb:       eval_acc/fold15_accuracy 50
wandb:       eval_acc/fold16_accuracy 36.65087
wandb:       eval_acc/fold17_accuracy 59.94021
wandb:       eval_acc/fold18_accuracy 68.23362
wandb:       eval_acc/fold19_accuracy 67.94872
wandb:        eval_acc/fold1_accuracy 84.61538
wandb:       eval_acc/fold20_accuracy 48.74372
wandb:       eval_acc/fold21_accuracy 38.31418
wandb:       eval_acc/fold22_accuracy 59.00901
wandb:        eval_acc/fold2_accuracy 53.12024
wandb:        eval_acc/fold3_accuracy 69.6845
wandb:        eval_acc/fold4_accuracy 57.69231
wandb:        eval_acc/fold5_accuracy 41.99706
wandb:        eval_acc/fold6_accuracy 42.25589
wandb:        eval_acc/fold7_accuracy 36.36364
wandb:        eval_acc/fold8_accuracy 52.47525
wandb:        eval_acc/fold9_accuracy 49.07975
wandb:               eval_f1/fold0_f1 0.77148
wandb:              eval_f1/fold10_f1 0.3237
wandb:              eval_f1/fold11_f1 0.37935
wandb:              eval_f1/fold12_f1 0.37639
wandb:              eval_f1/fold13_f1 0.62792
wandb:              eval_f1/fold14_f1 0.6022
wandb:              eval_f1/fold15_f1 0.46936
wandb:              eval_f1/fold16_f1 0.34895
wandb:              eval_f1/fold17_f1 0.62312
wandb:              eval_f1/fold18_f1 0.68159
wandb:              eval_f1/fold19_f1 0.67983
wandb:               eval_f1/fold1_f1 0.83258
wandb:              eval_f1/fold20_f1 0.47234
wandb:              eval_f1/fold21_f1 0.38652
wandb:              eval_f1/fold22_f1 0.59117
wandb:               eval_f1/fold2_f1 0.48431
wandb:               eval_f1/fold3_f1 0.62231
wandb:               eval_f1/fold4_f1 0.52375
wandb:               eval_f1/fold5_f1 0.4661
wandb:               eval_f1/fold6_f1 0.33134
wandb:               eval_f1/fold7_f1 0.30514
wandb:               eval_f1/fold8_f1 0.44485
wandb:               eval_f1/fold9_f1 0.45236
wandb:           eval_loss/fold0_loss 0.98733
wandb:          eval_loss/fold10_loss 2.84445
wandb:          eval_loss/fold11_loss 2.94568
wandb:          eval_loss/fold12_loss 3.41386
wandb:          eval_loss/fold13_loss 1.35887
wandb:          eval_loss/fold14_loss 1.79375
wandb:          eval_loss/fold15_loss 2.19595
wandb:          eval_loss/fold16_loss 2.72892
wandb:          eval_loss/fold17_loss 1.37327
wandb:          eval_loss/fold18_loss 1.49108
wandb:          eval_loss/fold19_loss 1.22475
wandb:           eval_loss/fold1_loss 0.46424
wandb:          eval_loss/fold20_loss 2.10448
wandb:          eval_loss/fold21_loss 3.34889
wandb:          eval_loss/fold22_loss 1.99689
wandb:           eval_loss/fold2_loss 2.26554
wandb:           eval_loss/fold3_loss 1.69786
wandb:           eval_loss/fold4_loss 2.55875
wandb:           eval_loss/fold5_loss 3.00568
wandb:           eval_loss/fold6_loss 3.83753
wandb:           eval_loss/fold7_loss 2.22447
wandb:           eval_loss/fold8_loss 2.43939
wandb:           eval_loss/fold9_loss 2.33596
wandb:                 train/fold0_lr 8e-05
wandb:                train/fold10_lr 8e-05
wandb:                train/fold11_lr 8e-05
wandb:                train/fold12_lr 8e-05
wandb:                train/fold13_lr 8e-05
wandb:                train/fold14_lr 8e-05
wandb:                train/fold15_lr 8e-05
wandb:                train/fold16_lr 8e-05
wandb:                train/fold17_lr 8e-05
wandb:                train/fold18_lr 8e-05
wandb:                train/fold19_lr 8e-05
wandb:                 train/fold1_lr 8e-05
wandb:                train/fold20_lr 8e-05
wandb:                train/fold21_lr 8e-05
wandb:                train/fold22_lr 8e-05
wandb:                 train/fold2_lr 8e-05
wandb:                 train/fold3_lr 8e-05
wandb:                 train/fold4_lr 8e-05
wandb:                 train/fold5_lr 8e-05
wandb:                 train/fold6_lr 8e-05
wandb:                 train/fold7_lr 8e-05
wandb:                 train/fold8_lr 8e-05
wandb:                 train/fold9_lr 8e-05
wandb:  train_accuracy/fold0_accuracy 100
wandb: train_accuracy/fold10_accuracy 100
wandb: train_accuracy/fold11_accuracy 100
wandb: train_accuracy/fold12_accuracy 99.93861
wandb: train_accuracy/fold13_accuracy 100
wandb: train_accuracy/fold14_accuracy 100
wandb: train_accuracy/fold15_accuracy 100
wandb: train_accuracy/fold16_accuracy 100
wandb: train_accuracy/fold17_accuracy 100
wandb: train_accuracy/fold18_accuracy 100
wandb: train_accuracy/fold19_accuracy 100
wandb:  train_accuracy/fold1_accuracy 100
wandb: train_accuracy/fold20_accuracy 100
wandb: train_accuracy/fold21_accuracy 100
wandb: train_accuracy/fold22_accuracy 100
wandb:  train_accuracy/fold2_accuracy 100
wandb:  train_accuracy/fold3_accuracy 100
wandb:  train_accuracy/fold4_accuracy 100
wandb:  train_accuracy/fold5_accuracy 100
wandb:  train_accuracy/fold6_accuracy 100
wandb:  train_accuracy/fold7_accuracy 100
wandb:  train_accuracy/fold8_accuracy 100
wandb:  train_accuracy/fold9_accuracy 100
wandb:          train_loss/fold0_loss 0.02761
wandb:         train_loss/fold10_loss 0.02363
wandb:         train_loss/fold11_loss 0.01057
wandb:         train_loss/fold12_loss 0.01817
wandb:         train_loss/fold13_loss 0.03112
wandb:         train_loss/fold14_loss 0.03157
wandb:         train_loss/fold15_loss 0.03169
wandb:         train_loss/fold16_loss 0.01556
wandb:         train_loss/fold17_loss 0.02136
wandb:         train_loss/fold18_loss 0.02625
wandb:         train_loss/fold19_loss 0.03843
wandb:          train_loss/fold1_loss 0.03799
wandb:         train_loss/fold20_loss 0.02149
wandb:         train_loss/fold21_loss 0.023
wandb:         train_loss/fold22_loss 0.01505
wandb:          train_loss/fold2_loss 0.02026
wandb:          train_loss/fold3_loss 0.02566
wandb:          train_loss/fold4_loss 0.02157
wandb:          train_loss/fold5_loss 0.02713
wandb:          train_loss/fold6_loss 0.02368
wandb:          train_loss/fold7_loss 0.0252
wandb:          train_loss/fold8_loss 0.02529
wandb:          train_loss/fold9_loss 0.04387
```





##### 加medication

```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.60      0.70      0.65      1026
           1       0.53      0.47      0.50       828
           2       0.37      0.31      0.34       486

    accuracy                           0.54      2340
   macro avg       0.50      0.49      0.49      2340
weighted avg       0.53      0.54      0.53      2340

              precision    recall  f1-score   support

           0       0.69      0.74      0.71       675
           1       0.36      0.42      0.38       306
           2       0.47      0.29      0.35       252

    accuracy                           0.57      1233
   macro avg       0.50      0.48      0.48      1233
weighted avg       0.56      0.57      0.56      1233

              precision    recall  f1-score   support

           0       0.47      0.62      0.54       351
           1       0.68      0.51      0.58       522
           2       0.31      0.33      0.32       234

    accuracy                           0.51      1107
   macro avg       0.49      0.49      0.48      1107
weighted avg       0.54      0.51      0.51      1107

==========LAST REPORTS============
              precision    recall  f1-score   support

           0       0.54      0.70      0.61      1026
           1       0.44      0.35      0.39       828
           2       0.41      0.29      0.34       486

    accuracy                           0.49      2340
   macro avg       0.46      0.45      0.45      2340
weighted avg       0.48      0.49      0.48      2340

              precision    recall  f1-score   support

           0       0.65      0.73      0.69       675
           1       0.38      0.43      0.40       306
           2       0.49      0.24      0.32       252

    accuracy                           0.56      1233
   macro avg       0.51      0.47      0.47      1233
weighted avg       0.55      0.56      0.54      1233

              precision    recall  f1-score   support

           0       0.39      0.63      0.48       351
           1       0.52      0.31      0.39       522
           2       0.36      0.35      0.35       234

    accuracy                           0.42      1107
   macro avg       0.42      0.43      0.41      1107
weighted avg       0.44      0.42      0.41      1107
```



###### ➕optimizer改为AdamW

效果差不多，还是换回RMSprop。

```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.62      0.69      0.65      1026
           1       0.45      0.49      0.47       828
           2       0.36      0.23      0.28       486

    accuracy                           0.52      2340
   macro avg       0.48      0.47      0.47      2340
weighted avg       0.51      0.52      0.51      2340

              precision    recall  f1-score   support

           0       0.73      0.74      0.74       675
           1       0.36      0.44      0.40       306
           2       0.40      0.29      0.33       252

    accuracy                           0.57      1233
   macro avg       0.50      0.49      0.49      1233
weighted avg       0.57      0.57      0.57      1233

              precision    recall  f1-score   support

           0       0.45      0.58      0.51       351
           1       0.51      0.52      0.51       522
           2       0.30      0.17      0.21       234

    accuracy                           0.46      1107
   macro avg       0.42      0.42      0.41      1107
weighted avg       0.45      0.46      0.45      1107

==========LAST REPORTS============
              precision    recall  f1-score   support

           0       0.59      0.63      0.61      1026
           1       0.45      0.33      0.38       828
           2       0.18      0.24      0.21       486

    accuracy                           0.44      2340
   macro avg       0.41      0.40      0.40      2340
weighted avg       0.46      0.44      0.44      2340

              precision    recall  f1-score   support

           0       0.73      0.68      0.71       675
           1       0.42      0.36      0.39       306
           2       0.20      0.27      0.23       252

    accuracy                           0.52      1233
   macro avg       0.45      0.44      0.44      1233
weighted avg       0.55      0.52      0.53      1233

              precision    recall  f1-score   support

           0       0.40      0.54      0.46       351
           1       0.48      0.30      0.37       522
           2       0.16      0.21      0.18       234

    accuracy                           0.36      1107
   macro avg       0.35      0.35      0.34      1107
weighted avg       0.39      0.36      0.36      1107

```





### batch=32（+medication)

结果依然差不多，batch_size影响不大。

```
=========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.60      0.70      0.65      1026
           1       0.53      0.47      0.50       828
           2       0.37      0.31      0.34       486

    accuracy                           0.54      2340
   macro avg       0.50      0.49      0.49      2340
weighted avg       0.53      0.54      0.53      2340

              precision    recall  f1-score   support

           0       0.69      0.74      0.71       675
           1       0.36      0.42      0.38       306
           2       0.47      0.29      0.35       252

    accuracy                           0.57      1233
   macro avg       0.50      0.48      0.48      1233
weighted avg       0.56      0.57      0.56      1233

              precision    recall  f1-score   support

           0       0.47      0.62      0.54       351
           1       0.68      0.51      0.58       522
           2       0.31      0.33      0.32       234

    accuracy                           0.51      1107
   macro avg       0.49      0.49      0.48      1107
weighted avg       0.54      0.51      0.51      1107

==========LAST REPORTS============
              precision    recall  f1-score   support

           0       0.54      0.70      0.61      1026
           1       0.44      0.35      0.39       828
           2       0.41      0.29      0.34       486

    accuracy                           0.49      2340
   macro avg       0.46      0.45      0.45      2340
weighted avg       0.48      0.49      0.48      2340

              precision    recall  f1-score   support

           0       0.65      0.73      0.69       675
           1       0.38      0.43      0.40       306
           2       0.49      0.24      0.32       252

    accuracy                           0.56      1233
   macro avg       0.51      0.47      0.47      1233
weighted avg       0.55      0.56      0.54      1233

              precision    recall  f1-score   support

           0       0.39      0.63      0.48       351
           1       0.52      0.31      0.39       522
           2       0.36      0.35      0.35       234

    accuracy                           0.42      1107
   macro avg       0.42      0.43      0.41      1107
weighted avg       0.44      0.42      0.41      1107

```



