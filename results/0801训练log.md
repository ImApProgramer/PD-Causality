# 0807
## PoseFormerV2
### Baseline
```
python eval_encoder.py --backbone poseformerv2 --medication 1 --train_mode end2end
```

相比于0801第一次运行的时候没有任何改动。

训练结果如下：
```
==========BEST REPORTS============
              precision    recall  f1-score   support

           0       0.45      0.57      0.50      1005
           1       0.27      0.21      0.24       828
           2       0.30      0.25      0.27       483

    accuracy                           0.37      2316
   macro avg       0.34      0.34      0.34      2316
weighted avg       0.35      0.37      0.36      2316

              precision    recall  f1-score   support

           0       0.62      0.68      0.65       654
           1       0.23      0.21      0.22       306
           2       0.29      0.26      0.27       249

    accuracy                           0.47      1209
   macro avg       0.38      0.38      0.38      1209
weighted avg       0.45      0.47      0.46      1209

              precision    recall  f1-score   support

           0       0.23      0.38      0.29       351
           1       0.30      0.21      0.25       522
           2       0.31      0.24      0.27       234

    accuracy                           0.27      1107
   macro avg       0.28      0.27      0.27      1107
weighted avg       0.28      0.27      0.26      1107

==========LAST REPORTS============
              precision    recall  f1-score   support

           0       0.54      0.52      0.53      1005
           1       0.33      0.30      0.32       828
           2       0.19      0.24      0.21       483

    accuracy                           0.38      2316
   macro avg       0.36      0.35      0.35      2316
weighted avg       0.39      0.38      0.39      2316

              precision    recall  f1-score   support

           0       0.68      0.60      0.64       654
           1       0.34      0.36      0.35       306
           2       0.24      0.29      0.26       249

    accuracy                           0.48      1209
   macro avg       0.42      0.42      0.42      1209
weighted avg       0.50      0.48      0.49      1209

              precision    recall  f1-score   support

           0       0.33      0.36      0.34       351
           1       0.33      0.27      0.29       522
           2       0.14      0.18      0.16       234

    accuracy                           0.28      1107
   macro avg       0.27      0.27      0.27      1107
weighted avg       0.29      0.28      0.28      1107

wandb: Run summary:
wandb:                          epoch 19
wandb:        eval_acc/fold0_accuracy 45.4844
wandb:       eval_acc/fold10_accuracy 34.80663
wandb:       eval_acc/fold11_accuracy 35.71429
wandb:       eval_acc/fold12_accuracy 45.99709
wandb:       eval_acc/fold13_accuracy 38.0303
wandb:       eval_acc/fold14_accuracy 27.60646
wandb:       eval_acc/fold15_accuracy 44.68864
wandb:       eval_acc/fold16_accuracy 39.49447
wandb:       eval_acc/fold17_accuracy 53.66218
wandb:       eval_acc/fold18_accuracy 47.4359
wandb:       eval_acc/fold19_accuracy 57.93304
wandb:        eval_acc/fold1_accuracy 37.19298
wandb:       eval_acc/fold20_accuracy 37.68844
wandb:       eval_acc/fold21_accuracy 35.67251
wandb:       eval_acc/fold22_accuracy 43.24324
wandb:        eval_acc/fold2_accuracy 37.92049
wandb:        eval_acc/fold3_accuracy 36.28692
wandb:        eval_acc/fold4_accuracy 40.90247
wandb:        eval_acc/fold5_accuracy 29.66226
wandb:        eval_acc/fold6_accuracy 21.99662
wandb:        eval_acc/fold7_accuracy 53.26954
wandb:        eval_acc/fold8_accuracy 30.28765
wandb:        eval_acc/fold9_accuracy 55.41667
wandb:               eval_f1/fold0_f1 0.42476
wandb:              eval_f1/fold10_f1 0.35411
wandb:              eval_f1/fold11_f1 0.40685
wandb:              eval_f1/fold12_f1 0.37412
wandb:              eval_f1/fold13_f1 0.37767
wandb:              eval_f1/fold14_f1 0.28416
wandb:              eval_f1/fold15_f1 0.42507
wandb:              eval_f1/fold16_f1 0.35641
wandb:              eval_f1/fold17_f1 0.51389
wandb:              eval_f1/fold18_f1 0.48988
wandb:              eval_f1/fold19_f1 0.56903
wandb:               eval_f1/fold1_f1 0.36759
wandb:              eval_f1/fold20_f1 0.36822
wandb:              eval_f1/fold21_f1 0.37
wandb:              eval_f1/fold22_f1 0.37458
wandb:               eval_f1/fold2_f1 0.35983
wandb:               eval_f1/fold3_f1 0.34471
wandb:               eval_f1/fold4_f1 0.40123
wandb:               eval_f1/fold5_f1 0.37295
wandb:               eval_f1/fold6_f1 0.23331
wandb:               eval_f1/fold7_f1 0.41425
wandb:               eval_f1/fold8_f1 0.30976
wandb:               eval_f1/fold9_f1 0.4938
wandb:           eval_loss/fold0_loss 1.11436
wandb:          eval_loss/fold10_loss 1.13292
wandb:          eval_loss/fold11_loss 1.35604
wandb:          eval_loss/fold12_loss 1.16137
wandb:          eval_loss/fold13_loss 1.01841
wandb:          eval_loss/fold14_loss 1.21189
wandb:          eval_loss/fold15_loss 1.19656
wandb:          eval_loss/fold16_loss 1.22022
wandb:          eval_loss/fold17_loss 0.96646
wandb:          eval_loss/fold18_loss 1.13613
wandb:          eval_loss/fold19_loss 0.91031
wandb:           eval_loss/fold1_loss 1.18906
wandb:          eval_loss/fold20_loss 1.06428
wandb:          eval_loss/fold21_loss 1.22453
wandb:          eval_loss/fold22_loss 1.19183
wandb:           eval_loss/fold2_loss 1.15225
wandb:           eval_loss/fold3_loss 1.2878
wandb:           eval_loss/fold4_loss 1.23459
wandb:           eval_loss/fold5_loss 1.21759
wandb:           eval_loss/fold6_loss 1.42411
wandb:           eval_loss/fold7_loss 1.00479
wandb:           eval_loss/fold8_loss 1.15485
wandb:           eval_loss/fold9_loss 0.92878
wandb:                 train/fold0_lr 8e-05
wandb:                train/fold10_lr 8e-05
wandb:                train/fold11_lr 8e-05
wandb:                train/fold12_lr 8e-05
wandb:                train/fold13_lr 8e-05
wandb:                train/fold14_lr 8e-05
wandb:                train/fold15_lr 8e-05
wandb:                train/fold16_lr 8e-05
wandb:                train/fold17_lr 8e-05
wandb:                train/fold18_lr 8e-05
wandb:                train/fold19_lr 8e-05
wandb:                 train/fold1_lr 8e-05
wandb:                train/fold20_lr 8e-05
wandb:                train/fold21_lr 8e-05
wandb:                train/fold22_lr 8e-05
wandb:                 train/fold2_lr 8e-05
wandb:                 train/fold3_lr 8e-05
wandb:                 train/fold4_lr 8e-05
wandb:                 train/fold5_lr 8e-05
wandb:                 train/fold6_lr 8e-05
wandb:                 train/fold7_lr 8e-05
wandb:                 train/fold8_lr 8e-05
wandb:                 train/fold9_lr 8e-05
wandb:  train_accuracy/fold0_accuracy 99.1128
wandb: train_accuracy/fold10_accuracy 97.86845
wandb: train_accuracy/fold11_accuracy 98.7005
wandb: train_accuracy/fold12_accuracy 98.55799
wandb: train_accuracy/fold13_accuracy 95.77855
wandb: train_accuracy/fold14_accuracy 96.94758
wandb: train_accuracy/fold15_accuracy 98.77825
wandb: train_accuracy/fold16_accuracy 98.77419
wandb: train_accuracy/fold17_accuracy 88.63636
wandb: train_accuracy/fold18_accuracy 97.71352
wandb: train_accuracy/fold19_accuracy 97.37569
wandb:  train_accuracy/fold1_accuracy 96.81934
wandb: train_accuracy/fold20_accuracy 96.40152
wandb: train_accuracy/fold21_accuracy 97.19068
wandb: train_accuracy/fold22_accuracy 97.87658
wandb:  train_accuracy/fold2_accuracy 95.46685
wandb:  train_accuracy/fold3_accuracy 97.99223
wandb:  train_accuracy/fold4_accuracy 97.17552
wandb:  train_accuracy/fold5_accuracy 96.35488
wandb:  train_accuracy/fold6_accuracy 92.82511
wandb:  train_accuracy/fold7_accuracy 95.6117
wandb:  train_accuracy/fold8_accuracy 97.41769
wandb:  train_accuracy/fold9_accuracy 98.4994
wandb:          train_loss/fold0_loss 0.4442
wandb:         train_loss/fold10_loss 0.55472
wandb:         train_loss/fold11_loss 0.43354
wandb:         train_loss/fold12_loss 0.46021
wandb:         train_loss/fold13_loss 0.62894
wandb:         train_loss/fold14_loss 0.5398
wandb:         train_loss/fold15_loss 0.42477
wandb:         train_loss/fold16_loss 0.49941
wandb:         train_loss/fold17_loss 0.65796
wandb:         train_loss/fold18_loss 0.5076
wandb:         train_loss/fold19_loss 0.55425
wandb:          train_loss/fold1_loss 0.57655
wandb:         train_loss/fold20_loss 0.59506
wandb:         train_loss/fold21_loss 0.47138
wandb:         train_loss/fold22_loss 0.48774
wandb:          train_loss/fold2_loss 0.52297
wandb:          train_loss/fold3_loss 0.475
wandb:          train_loss/fold4_loss 0.52913
wandb:          train_loss/fold5_loss 0.51169
wandb:          train_loss/fold6_loss 0.59424
wandb:          train_loss/fold7_loss 0.64564
wandb:          train_loss/fold8_loss 0.5226
wandb:          train_loss/fold9_loss 0.50187
```

### 不加入medication
```
python eval_encoder.py --backbone poseformerv2  --train_mode end2end
```

### 更改epoch到30
`python eval_encoder.py --backbone poseformerv2  --train_mode end2end`



## CTR-GCN
Baseline:
